---
title: Forcasting US Elections using Polls-of-Polls
subtitle: "My subtitle if needed"
author: 
  - Shamayla Durrin
  - Denise Chang 
  - Krishna Kumar
thanks: "Code and data are available at: [https://github.com/krishnak30/US_elections](https://github.com/krishnak30/US_elections)."

date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false
library(psych)
library(tidyverse)
library(arrow)
library(here)
library(ggplot2)
library(kableExtra)
library(sf)
library(tidyverse)
library(plotly)
library(RColorBrewer)
library(stringr)
library(modelsummary)
library(broom)
library(sf)
analysis_data <- read_parquet(here("data/02-analysis_data/analysis_data.parquet"))
mapping_data <- st_read(here("data/03-shapefiles_data/US"))
mean_support_data <- read_parquet(here("data/02-analysis_data/mapping_data.parquet"))
```

# Introduction

# Data

### Measurement

In the dataset of our analysis, the process of measurement begins by capturing real-world public opinion through surveys. Polling organizations transform this observed phenomenon into structured data by recording support percentages, pollster details, and methodological choices, thus turning public sentiment into quantifiable entries in the dataset.

Public opinion polls are essential for understanding voter preferences and electoral dynamics, providing data that informs policy decisions and campaign strategies. Our data set captures public support for candidates at specific times, along with information on methodologies, sample sizes, and pollster ratings.

Polling organizations use various methods, such as App panels, IVR (Interactive Voice Response), Live phone calls, Text-to-web systems etc. These methods can influence the validity and reliability of results, with larger sample sizes generally yielding more reliable estimates. Some methods, like live phone interviews, may be seen as more trustworthy than others.

The dataset also includes important measures of pollster performance, such as numeric grades, poll scores, and transparency scores. These metrics are calculated by considering bias, race difficulty, predictive error, and transparency, allowing us to assess the accuracy of each poll. The "pollscore," for instance, reflects a pollster’s past performance and reliability, adjusted for potential biases.

### Data Cleaning

The raw data for this project, sourced from FiveThirtyEight, [@FiveThirtyEight] underwent a series of cleaning steps to prepare it for analysis. Initially, duplicate rows were removed to ensure that only unique observations remained, facilitated by the janitor package \[\@janitor\]. A new binary variable, 'national', was created to indicate whether a poll was conducted at the national or state level. Missing values in the 'state' column were replaced with "Not Applicable," and numeric grades were evaluated to filter out low-quality pollsters, keeping only those with a numeric grade above 1. This cutoff was selected to retain mid to high-level pollsters for more reliable results. These steps were performed using functions from the dpylr package \[\@dplyr\]. Furthermore, dates were standardized and converted into a proper format for analysis using the lubridate package \[\@lubridate\]. Polls related to Kamala Harris were retained for further analysis, and percentage support values were transformed into actual numbers of supporters based on sample size. Additionally, pollster counts below five were excluded to focus on more reliable data sources. Polls regarding Kamala Harris were filtered to include only those conducted after her official candidacy announcement on July 21, 2024, ensuring the data reflects post-announcement public sentiment.The cleaned dataset was saved in Parquet format for efficient storage and retrieval, using the arrow package \[\@arrow\].

### Summary of Variables of Interest

```{r}
# Assuming you have the dataset numeric_data already loaded
#| include: false
#| warning: false
#| message: false


numeric_data <- analysis_data %>% select(numeric_grade, pollscore, sample_size, pct, num_support)

# Create a summary table
summary_table <- numeric_data %>%
  reframe(
    `Variable` = c("numeric_grade", "pollscore", "sample_size", "pct", "num_support"),
    `Mean` = c(mean(numeric_grade, na.rm = TRUE), 
               mean(pollscore, na.rm = TRUE), 
               mean(sample_size, na.rm = TRUE), 
               mean(pct, na.rm = TRUE), 
               mean(num_support, na.rm = TRUE)),
    `Max` = c(max(numeric_grade, na.rm = TRUE), 
              max(pollscore, na.rm = TRUE), 
              max(sample_size, na.rm = TRUE), 
              max(pct, na.rm = TRUE), 
              max(num_support, na.rm = TRUE)),
    `Min` = c(min(numeric_grade, na.rm = TRUE), 
              min(pollscore, na.rm = TRUE), 
              min(sample_size, na.rm = TRUE), 
              min(pct, na.rm = TRUE), 
              min(num_support, na.rm = TRUE)),
    `Standard Deviation` = c(sd(numeric_grade, na.rm = TRUE), 
                             sd(pollscore, na.rm = TRUE), 
                             sd(sample_size, na.rm = TRUE), 
                             sd(pct, na.rm = TRUE), 
                             sd(num_support, na.rm = TRUE))
  )

kable(summary_table)
```

## Variables

The data set contains a wide range of variables that are essential for understanding polling outcomes. The following are key variables:

-   **`poll_id`**: Unique identifier for each poll conducted.

-   **`pollster`**: The polling organization that conducted the poll (e.g., YouGov, TIPP).

-   **`numeric_grade`**: A numeric rating indicating the pollster’s quality or reliability (e.g., 1.8).

-   **`pollscore`**: A score representing the reliability of the pollster in terms of error and bias.

-   **`state`**: The state where the poll was conducted (or "Not Applicable" for national polls).

-   **`start_date` / `end_date`**: Dates during which the poll was conducted.

-   **`sample_size`**: The number of respondents in the poll.

-   **`population`**: The group surveyed (e.g., likely voters or registered voters).

-   **`candidate_name`**: The name of the candidate being polled (e.g., Kamala Harris, Donald Trump).

-   **`pct`**: The percentage of support for the candidate in the poll.

-   **`methodology`**: The method used to conduct the poll (e.g. online panel)

-   **`transparency_score`**: A grade for how transparent a pollster is, calculated based on how much information it discloses about its polls and weighted by recency. The highest Transparency Score is 10.

## Summary Statistics

### 1) Average Pollster Quality (pollscore): mean, median, mode, min, max

```{r}
library(dplyr)

# Generate summary statistics for national and state polls
summary_stats <- analysis_data %>%
  filter(!is.na(pollscore)) %>%  # Exclude rows with missing pollscore
  group_by(national) %>%        # Group by poll_scope
  summarize(
    Count = n(),                   
    Min = round(min(pollscore), 2),
    `1st Qu.` = round(quantile(pollscore, 0.25), 2),
    Median = round(median(pollscore), 2),
    Mean = round(mean(pollscore), 2),
    `3rd Qu.` = round(quantile(pollscore, 0.75), 2),
    Max = round(max(pollscore), 2),
    .groups = 'drop'  # Drop grouping structure for cleaner output
  )

# Print the summary statistics
kable(summary_stats)


```

As shown above, the summary statistics table **\[CROSS-REFERENCE HERE\]** offers an overview of poll scores for national and state polls, with 2,971 and 2,953 entries, respectively. Both categories feature a minimum poll score of -1.5, which a high level of validity and reliability. The first quartile shows national polls at -0.9, while state polls are slightly lower at -1.1. Median scores are -0.3 for national polls and -0.4 for state polls, suggesting that half of the polls in both categories yield moderately negative scores, reinforcing a trend of favorable sentiment. The mean score is -0.37 for national polls, indicating a less reliable perception compared to the -0.51 mean for state polls. The third quartile for national polls (0.0) indicates that the upper 25% of scores are neutral, while state polls remain just below neutral at 0.7. The maximum score of 1.7 for national polls reflects a notable negative perception for at least one candidate, while the maximum for state polls is 0.7. Overall, these statistics illustrate a strong reliability of pollsters considering their errors and bias.

```         
```

### 2) Distribution of sample sizes: same as above

```{r}
library(ggplot2)
library(dplyr)
library(gridExtra)  # Load the gridExtra package

# Create a new variable for sample size ranges
analysis_data <- analysis_data %>%
  mutate(sample_size_range = case_when(
    sample_size <= 10000 ~ "0-10,000",
    sample_size > 10000 & sample_size <= 20000 ~ "10,001-20,000",
    sample_size > 20000 & sample_size <= 30000 ~ "20,001-30,000",
    TRUE ~ "Above 30,000"
  ))

# Plot for 0-10,000
p1 <- ggplot(analysis_data %>% filter(sample_size <= 10000), aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "lightgreen", color = NA, alpha = 0.7) +
  labs(title = "Sample Sizes 
       (0-10,000)",
       x = "Sample Size",
       y = "Count") +
  scale_x_continuous(limits = c(0, 10000)) +  # Set x limits for the first plot
  scale_y_continuous(limits = c(0, 2300)) +  # Set y limits for the first plot
  theme_minimal() +
  theme(plot.title = element_text(size = 10),  # **Decreased font size of the title**
        axis.title.y = element_text(margin = margin(r = 15)))  # **Increased gap for y-axis label**

# Plot for 10,001 - 20,000
p2 <- ggplot(analysis_data %>% filter(sample_size > 10000 & sample_size <= 20000), aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "indianred", color = NA, alpha = 0.7) +
  labs(title = "Sample Sizes 
       (10,001-20,000)",
       x = "Sample Size",
       y = "Count") +
  scale_x_continuous(limits = c(10001, 20000)) +  # Set x limits for the second plot
  scale_y_continuous(limits = c(0, 50)) +  # Set y limits for the second plot
  theme_minimal() +
  theme(plot.title = element_text(size = 10),  # **Decreased font size of the title**
        axis.title.y = element_text(margin = margin(r = 10)))  # **Increased gap for y-axis label**

# Plot for 20,001 - 30,000
p3 <- ggplot(analysis_data %>% filter(sample_size > 20000 & sample_size <= 30000), aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "lightblue", color = NA, alpha = 0.7) +
  labs(title = "Sample Sizes 
       (20,001-30,000)",
       x = "Sample Size",
       y = "Count") +
  scale_x_continuous(limits = c(20001, 30000)) +  # Set x limits for the third plot
  scale_y_continuous(limits = c(0, 10)) +  # Set y limits for the third plot
  theme_minimal() +
  theme(plot.title = element_text(size = 10),  # **Decreased font size of the title**
        axis.title.y = element_text(margin = margin(r = 5)))  # **Increased gap for y-axis label**

# Arrange the plots side by side
grid.arrange(p1, p2, p3, ncol = 3)  # ncol=3 arranges the plots in one row


```

The histograms **\[CROSS-REFERENCE HERE\]** reveal significant trends in sample sizes across the different ranges, indicating a general tendency towards smaller samples in the polling data. Most values within the 0-10,000 range cluster at the lower end, suggesting that many polls are conducted with limited respondent numbers, which may compromise the reliability and representativeness of the results. **However, this conclusion is very much dependent on what we determine to be an acceptable sample size for the results to be considered reliable**. In the 10,001-20,000 range, while there is some increase in sample sizes, the majority still falls within the 10,000-12,500 bracket, with few reaching the upper limits.

The findings from the 20,001-30,000 range further illustrate the scarcity of larger sample sizes, with only a couple of values noted at each of the significant points. This distribution emphasizes a preference for smaller samples in the polling data analyzed.

In the context of polling results, these findings suggest potential limitations in the accuracy and validity of the insights generated. Larger sample sizes are typically preferred in polling as they tend to yield more reliable estimates and reduce the margin of error, leading to a better representation of the population's views. Consequently, the predominance of smaller sample sizes in this data set may raise concerns about the robustness of the polling findings, potentially affecting their applicability in broader analyses or decision-making processes. However, we must determine an acceptable and ideal sample size that would allow us to determine the reliability of these results.

# Forecasting Approach Using Polls of Polls Method

The polls of polls methodology is widely used in election prediction as it aggregates multiple polls to provide a more reliable estimate of voter support, rather than relying on any single poll. The goal is to reduce errors and biases present in individual polls by using a weighted average of many different polls.

In our approach, we will employ linear modeling of voter support percentage (pct) on pollster and other independent variables such as sample size, poll recency, and poll scope (state vs. national). This will allow us to smooth out the inherent noise, biases, and variability across different pollsters. Once we obtain the predicted values from our model, we will weight these predictions based on the numeric grade (quality score) of each pollster to calculate an overall national estimate of the outcome. Additionally, we will separately compute estimates for key battleground states, where voter behavior can be more volatile and pivotal in deciding the final outcome of the election. This approach helps us capture both national trends and critical state-level dynamics.

## **Exploration of Independent Variables**

### Support for Kamala Harris by Sample Size and Polling Level

\[\@fig-sample\] suggests that the relationship between support for Kamala Harris and sample size shows a slight positive trend, indicating a small increase in support as sample sizes grow. Both national (blue) and state (green) polls reflect this upward trend, although the effect is minimal. Since voting outcomes often depend on small margins, this subtle increase in support with larger sample sizes could still be relevant in predicting overall support.

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: fig-sample
#| fig-cap: Sample Size vs. Support for Kamala Harris, Colored by Pollscope (National vs. State)

# Custom colors for pollscope
national_color <- "chartreuse3"
state_color <- "steelblue"

# Create the plot for Kamala Harris with the requested adjustments
ggplot(analysis_data %>% filter(candidate_name == "Kamala Harris"), 
       aes(x = sample_size, y = pct, color = factor(national))) +
  geom_point(size = 0.5, alpha = 0.9) +  # Smaller dots with slight transparency
  geom_smooth(aes(group = 1), method = "lm", se = FALSE, color = "azure4") +  # Linear model smoothing
  
  # Custom colors for national (1 for National, 0 for State)
  scale_color_manual(values = c("1" = national_color, "0" = state_color), 
                     labels = c("National", "State")) +  # Pollscope categories with custom colors
  
  # Same minimal styling
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.title = element_blank(),  # No title for the legend
    legend.position = "bottom",  # Move legend to the bottom
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(x = "Sample Size", y = "Support (%)")  # No title, just axis labels

```

### Variation in Support for Kamala Harris Among Most Frequent Pollsters

[@fig-top5pollsters] above shows the variation in support for Kamala Harris across the five most frequent pollsters. Notably, the median support levels differ between pollsters, indicating variability in central estimates of Harris’s support. For instance, Siena/NYT reports a higher median compared to Redfield & Wilton Strategies. Additionally, the differing ranges of support estimates highlight potential discrepancies in the methodologies, sample sizes, and biases employed by each pollster. This suggests that different pollster have different interpretations of candidate support, underlining the importance of aggregating data from multiple sources to avoid over-reliance on any single pollster.

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: fig-top5pollsters
#| fig-cap: "Support for Kamala Harris across the top 5 pollsters by poll count."

# Get top 5 pollsters by poll count
top5_pollsters <- analysis_data %>%
  filter(candidate_name == "Kamala Harris") %>%
  count(pollster, sort = TRUE) %>%
  top_n(5, n) %>%
  pull(pollster)

# Filter the data for only these top 5 pollsters
top5_data <- analysis_data %>%
  filter(pollster %in% top5_pollsters & candidate_name == "Kamala Harris")

# Custom colors for boxplot fill
boxplot_colors <- c("chartreuse3", "steelblue", "indianred", "purple", "darkorange")

# Create the boxplots with thinner borders, smaller points, and rotated x-axis labels
ggplot(top5_data, aes(x = pollster, y = pct, fill = pollster)) +
  geom_boxplot(size = 0.2, outlier.size = 0.5) +  # Thinner box lines and smaller outliers
  scale_fill_manual(values = boxplot_colors) +
  theme_minimal() +  # Minimal theme
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.position = "none",  # No legend needed for boxplots
    axis.line = element_line(color = "black"),  # Add axis lines
    plot.title = element_text(hjust = 0.5),  # Center title
    axis.text.x = element_text(angle = 20, hjust = 1),  # Rotate x-axis labels for better readability
    axis.line.y.right = element_line(color = "black"),  # Add right spine for clean look
    axis.line.x.top = element_blank(),  # Remove top spine
    panel.border = element_blank()  # Remove panel border
  ) +
  labs(x = "Pollster", y = "Support (%)")  # Labels

```

### Support for Kamala Harris Based on Recency of Polls and Pollster Quality

In \[\@fig-enddate\], we observe a slight increasing trend in support for Kamala Harris as the end date of polls progresses towards October. This may indicate a gradual improvement in her polling performance over time. The color gradient, reflecting the pollster's error and bias (Pollscore), reveals variations in bias: greener points represent more negative biases, while redder points indicate more positive biases.

While the recency of the polls shows a stable trend in Harris's support, the wide distribution of bias (Pollscore) values suggests that pollster quality can vary substantially. These variations could influence the model's predictions if left unaddressed. Hence, including both recency and pollster quality as predictor variables in the model might be valuable to account for such variations. Further model diagnostics and validation will help decide if these variables are significant.

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: fig-enddate
#| fig-cap: "Support for Kamala Harris based on End Date and colored by Pollster's Error and Bias."

# Define the color gradient based on pollscore
ggplot(analysis_data %>% filter(candidate_name == "Kamala Harris"), aes(x = end_date, y = pct, color = pollscore)) +
  geom_point(size = 0.8, alpha = 1) +  # Adjust point size and transparency
  geom_smooth(method = "lm", se = FALSE, color = "azure4") +  # Add a smooth trend line for overall support
  scale_color_gradient2(low = "blue", mid = "green", high = "red", midpoint = 0) +  # Color gradient
  theme_minimal() +  # Minimal theme as usual
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.position = "bottom",  # Legend at the bottom
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine for clean look
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(x = "Date Poll Ended",
       y = "Support (%)",
       color = "Error and Bias of Pollster (Low to High)")  # Updated label for color gradient

```

### Political Leanings of Different State

[@fig-mapinterac] provides a state-level representation of Kamala Harris's average support across the U.S., with the darker blue shades indicating states where her support is higher, and the redder tones showing where her support is lower. Gray areas represent states where we have no polling data available.

The gradient highlights notable geographic trends. Harris has the strongest support in states like Vermont, Massachusetts, and New York, while states like Utah and West Virginia show considerably lower levels of support. A distinct pattern emerges in the West, where coastal states like California favor Harris more strongly, contrasting with inland states like Wyoming and Utah.

This reveals that political leanings differ greatly by state, and this regional variation should be controlled for in any regression model analyzing poll data. The map serves as a useful tool for visualizing regional variations in voter support for Kamala Harris as reflected in the available polling data.

```{r}
# Filter out non-continental states (Hawaii and Alaska)
us_states_filtered <- mapping_data %>% 
  filter(!NAME %in% c("Alaska", "Hawaii"))

# Ensure correct merge between map data and Kamala's support data
map_data_full <- left_join(us_states_filtered, mean_support_data, by = c("NAME" = "state"))

# Create centroids for state labels
state_centroids <- st_centroid(us_states_filtered)

# Plot using geom_sf() for sf objects
p <- ggplot(map_data_full) +
  geom_sf(aes(fill = mean_support_kamala, text = paste("State:", str_to_title(NAME), "<br>Average Support:", round(mean_support_kamala, 2))), color = "white") +  # Capitalize state names
  scale_fill_distiller(palette = "RdBu", direction = 1, na.value = "gray", name = "Kamala's Support") +  # Reverse RdBu color gradient
  geom_text(data = state_centroids, aes(geometry = geometry, label = STUSPS), stat = "sf_coordinates", size = 2) +  # State labels
  labs(title = "Support for Kamala Harris by State", 
       caption = "States with no polling data are shown in gray") +
  theme_minimal() +
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5),
    panel.grid = element_blank(),  # Remove grid lines
    axis.title = element_blank(),  # No axis titles for maps
    axis.text = element_blank(),   # No axis labels
    axis.ticks = element_blank()   # No axis ticks
  )

# Turn ggplot to plotly for interactivity, add hover template for better control over display
ggplotly(p, tooltip = "text")

```

# Model

In this section, we aim to address the inherent biases and differences present in various polling data to arrive at a robust prediction model. The core challenge lies in selecting a model with an optimal balance between complexity and fit, ensuring it accurately captures the dynamics of polling data while avoiding overfitting. To this end, we carefully evaluated different model specifications to determine the most appropriate one for our forecasting purpose.

Given that variables like numeric grade and pollscore are perfectly collinear with pollster, they were excluded from the regression analysis to avoid multicollinearity issues. These variables, however, remain integral to our weighting strategy, where they will be used to adjust for differences in polling accuracy and reliability. Instead, we focus on key features such as pollster, sample size, state, and recency, gradually adding complexity to the model.

By systematically comparing model specifications that incorporate these variables, we aim to select the model with the right balance between predictive accuracy and generalizability, ultimately providing the best possible forecast.

## Model Set Up

We aim to model the percentage of support for Kamala Harris in each poll as a function of the pollster the sample size, the state, and the recency of the poll.

$$ y_i = \alpha + \beta_1 \cdot \texttt{pollster}_i + \beta_2 \cdot \texttt{sample_size}_i + \beta_3 \cdot \texttt{recency}_i +  \beta_4 \cdot \texttt{state}_i + \epsilon_i $$

Where

-   $y_i$​ is the percentage of support for Kamala Harris in poll iii,

-   $α$ is the intercept,

-   $β_1$ captures the effect of the polling organization,

-   $β_2​$ captures the effect of the sample size,

-   $β_3$​ captures the effect of recency (how recent the poll is),

-   $β_4$ capture the effects of the different states

-   ​ $\epsilon_i$ represents the error term, assumed to follow a normal distribution with mean 0.

## Model Justification

We compared three models with different sets of predictors, gradually increasing the model complexity. Model 1 only included **pollster** and **sample size** as predictors, while Model 2 introduced **state** as an additional factor. Finally, Model 3 incorporated **recency** as another key predictor. Among these models, Model 3 demonstrated the highest $R^2$ value (0.731), indicating the best fit and ability to explain the variation in the data. Additionally, Model 3 had the lowest AIC (4786.815) and BIC (5203.787) values, suggesting that it provided the best balance between model complexity and predictive performance. Therefore, Model 3 was selected as the most appropriate model for smoothing out polling biases and differences, offering the most accurate prediction of support for Kamala Harris.

```{r}
# Load necessary libraries
library(broom)
library(dplyr)
library(knitr)

# Load the models from RDS files
model_1 <- readRDS(here("models/model_1.rds"))
model_2 <- readRDS(here("models/model_2.rds"))
model_3 <- readRDS(here("models/model_3.rds"))

# Function to get model details
get_model_details <- function(model, model_name, include_recency = FALSE) {
  r2 <- summary(model)$r.squared
  adj_r2 <- summary(model)$adj.r.squared
  f_stat <- summary(model)$fstatistic[1]
  
  # Extract p-values for Sample Size and Recency
  sample_size_pvalue <- tidy(model) %>% filter(term == "sample_size") %>% pull(p.value)
  sample_size_significance <- ifelse(sample_size_pvalue < 0.05, paste0("Significant (p = ", round(sample_size_pvalue, 4), ")"), paste0("Not significant (p = ", round(sample_size_pvalue, 4), ")"))
  
  recency_significance <- "Not included"
  if (include_recency) {
    recency_pvalue <- tidy(model) %>% filter(term == "recency") %>% pull(p.value)
    recency_significance <- ifelse(recency_pvalue < 0.05, paste0("Highly significant (p < 2e-16)"), paste0("Not significant (p = ", round(recency_pvalue, 4), ")"))
  }
  
  return(data.frame(
    Model = model_name,
    Variables_Included = ifelse(include_recency, "Pollster + Sample Size + State + Recency", "Pollster + Sample Size + State"),
    R2 = round(r2, 3),
    Adjusted_R2 = round(adj_r2, 3),
    F_statistic = round(f_stat, 2),
    Sample_Size_Significance = sample_size_significance,
    Recency_Significance = recency_significance,
    States_Significant = ifelse(model_name == "Model 1", "Not included", ifelse(model_name == "Model 2", "Several states", "More states"))
  ))
}

# Generate the summaries for all models
model_1_summary <- get_model_details(model_1, "Model 1")
model_2_summary <- get_model_details(model_2, "Model 2")
model_3_summary <- get_model_details(model_3, "Model 3", include_recency = TRUE)

# Combine all summaries into one data frame
all_summaries <- bind_rows(model_1_summary, model_2_summary, model_3_summary)

# Print the table using kable for better formatting

kable(all_summaries, format = "markdown",row.names = FALSE, col.names = c("Model", "Variables Included", "R²", "Adjusted R²", "F-statistic", "Sample Size Significance", "Recency Significance", "States Significant"))


```

```{r}
# Create a data frame for AIC and BIC comparison
aic_bic_table <- data.frame(
  Model = c("Model 1", "Model 2", "Model 3"),
  AIC = c(AIC(model_1), AIC(model_2), AIC(model_3)),
  BIC = c(BIC(model_1), BIC(model_2), BIC(model_3))
)

# Print the table using kable for LaTeX format
kable(aic_bic_table, format = "markdown", col.names = c("Model", "AIC", "BIC"), booktabs = TRUE)

```

## Model Results

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Load necessary libraries
library(modelsummary)
library(broom)

# Load Model 3 from RDS file
model_3 <- readRDS(file = here::here("models/model_3.rds"))

# Generate LaTeX output for the model
latex_output <- modelsummary(model_3, output = "latex")

# Ensure the output is a character string
latex_output_string <- capture.output(cat(latex_output))

# Write the LaTeX string to a .tex file
writeLines(latex_output_string, here::here("model_3_summary.tex"))

```

```{r}
# Save LaTeX output to a string
latex_output <- modelsummary(model_3, output = "latex")

# Write the LaTeX string to a .tex file
writeLines(latex_output, here::here("output/model_3_summary.tex"))
```

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Load necessary libraries
library(modelsummary)
library(broom)

# Load Model 3 from RDS file
model_3 <- readRDS(file = here::here("models/model_3.rds"))

modelsummary(model_3, output = "latex", file = "model_3_summary.tex")
```

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

# Load necessary libraries
library(modelsummary)
library(broom)

# Load Model 3 from RDS file
model_3 <- readRDS(file = here::here("models/model_3.rds"))

modelsummary(model_3)

# Display the summary of Model 3 using modelsummary
#| echo: false
#| eval: true
#| label: tbl-model3results
#| tbl-cap: "Regression results for Model 3 including Pollster, Sample Size, State, and Recency"

modelsummary::modelsummary(
  list(
    "Model 3" = model_3
  ),
  statistic = "std.error",
  fmt = 2
)

```

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <-
  readRDS(file = here::here("models/first_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```

# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this.

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows...

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```

\newpage

# References
