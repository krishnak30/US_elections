---
title: Forcasting US Elections using Polls-of-Polls
subtitle: "My subtitle if needed"
author: 
  - Shamayla Durrin
  - Denise Chang 
  - Krishna Kumar
thanks: "Code and data are available at: [https://github.com/krishnak30/US_elections](https://github.com/krishnak30/US_elections)."

date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(here)
library(ggplot2)
analysis_data <- read_parquet(here("data/02-analysis_data/analysis_data.parquet"))
```

# Introduction

# Data

## Data Set Overview

add a summary statistics table here maybe?

```{r}
library(arrow)
analysis_data <- read_parquet(here::here("data/02-analysis_data/analysis_data.parquet"))

head(analysis_data, 20)

```

## Variables

The data set contains a wide range of variables that are essential for understanding polling outcomes. The following are key variables:

-   **`poll_id`**: Unique identifier for each poll conducted.

-   **`pollster`**: The polling organization that conducted the poll (e.g., YouGov, TIPP).

-   **`numeric_grade`**: A numeric rating indicating the pollsterâ€™s quality or reliability (e.g., 1.8).

-   **`pollscore`**: A score representing the reliability of the pollster in terms of error and bias.

-   **`state`**: The state where the poll was conducted (or "Not Applicable" for national polls).

-   **`start_date` / `end_date`**: Dates during which the poll was conducted.

-   **`sample_size`**: The number of respondents in the poll.

-   **`population`**: The group surveyed (e.g., likely voters or registered voters).

-   **`candidate_name`**: The name of the candidate being polled (e.g., Kamala Harris, Donald Trump).

-   **`pct`**: The percentage of support for the candidate in the poll.

-   **`methodology`**: The method used to conduct the poll (e.g. online panel)

-   **`transparency_score`**: A grade for how transparent a pollster is, calculated based on how much information it discloses about its polls and weighted by recency. The highest Transparency Score is 10.

## High Level Data Cleaning

## Summary Statistics

### 1) Average Pollster Quality (pollscore): mean, median, mode, min, max

```{r}
library(dplyr)

# Generate summary statistics for national and state polls
summary_stats <- analysis_data %>%
  filter(!is.na(pollscore)) %>%  # Exclude rows with missing pollscore
  group_by(poll_scope) %>%        # Group by poll_scope
  summarize(
    Count = n(),                   
    Min = round(min(pollscore), 2),
    `1st Qu.` = round(quantile(pollscore, 0.25), 2),
    Median = round(median(pollscore), 2),
    Mean = round(mean(pollscore), 2),
    `3rd Qu.` = round(quantile(pollscore, 0.75), 2),
    Max = round(max(pollscore), 2),
    .groups = 'drop'  # Drop grouping structure for cleaner output
  )

# Print the summary statistics
print(summary_stats)


```

As shown above, the summary statistics table **\[CROSS-REFERENCE HERE\]** offers an overview of poll scores for national and state polls, with 2,971 and 2,953 entries, respectively. Both categories feature a minimum poll score of -1.5, which a high level of validity and reliability. The first quartile shows national polls at -0.9, while state polls are slightly lower at -1.1. Median scores are -0.3 for national polls and -0.4 for state polls, suggesting that half of the polls in both categories yield moderately negative scores, reinforcing a trend of favorable sentiment. The mean score is -0.37 for national polls, indicating a less reliable perception compared to the -0.51 mean for state polls. The third quartile for national polls (0.0) indicates that the upper 25% of scores are neutral, while state polls remain just below neutral at 0.7. The maximum score of 1.7 for national polls reflects a notable negative perception for at least one candidate, while the maximum for state polls is 0.7. Overall, these statistics illustrate a strong reliability of pollsters considering their errors and bias.

### 2) Distribution of sample sizes: same as above

```{r}
library(ggplot2)
library(dplyr)
library(gridExtra)  # Load the gridExtra package

# Create a new variable for sample size ranges
analysis_data <- analysis_data %>%
  mutate(sample_size_range = case_when(
    sample_size <= 10000 ~ "0-10,000",
    sample_size > 10000 & sample_size <= 20000 ~ "10,001-20,000",
    sample_size > 20000 & sample_size <= 30000 ~ "20,001-30,000",
    TRUE ~ "Above 30,000"
  ))

# Plot for 0-10,000
p1 <- ggplot(analysis_data %>% filter(sample_size <= 10000), aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "lightgreen", color = NA, alpha = 0.7) +
  labs(title = "Sample Sizes 
       (0-10,000)",
       x = "Sample Size",
       y = "Count") +
  scale_x_continuous(limits = c(0, 10000)) +  # Set x limits for the first plot
  scale_y_continuous(limits = c(0, 2300)) +  # Set y limits for the first plot
  theme_minimal() +
  theme(plot.title = element_text(size = 10),  # **Decreased font size of the title**
        axis.title.y = element_text(margin = margin(r = 15)))  # **Increased gap for y-axis label**

# Plot for 10,001 - 20,000
p2 <- ggplot(analysis_data %>% filter(sample_size > 10000 & sample_size <= 20000), aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "indianred", color = NA, alpha = 0.7) +
  labs(title = "Sample Sizes 
       (10,001-20,000)",
       x = "Sample Size",
       y = "Count") +
  scale_x_continuous(limits = c(10001, 20000)) +  # Set x limits for the second plot
  scale_y_continuous(limits = c(0, 50)) +  # Set y limits for the second plot
  theme_minimal() +
  theme(plot.title = element_text(size = 10),  # **Decreased font size of the title**
        axis.title.y = element_text(margin = margin(r = 10)))  # **Increased gap for y-axis label**

# Plot for 20,001 - 30,000
p3 <- ggplot(analysis_data %>% filter(sample_size > 20000 & sample_size <= 30000), aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "lightblue", color = NA, alpha = 0.7) +
  labs(title = "Sample Sizes 
       (20,001-30,000)",
       x = "Sample Size",
       y = "Count") +
  scale_x_continuous(limits = c(20001, 30000)) +  # Set x limits for the third plot
  scale_y_continuous(limits = c(0, 10)) +  # Set y limits for the third plot
  theme_minimal() +
  theme(plot.title = element_text(size = 10),  # **Decreased font size of the title**
        axis.title.y = element_text(margin = margin(r = 5)))  # **Increased gap for y-axis label**

# Arrange the plots side by side
grid.arrange(p1, p2, p3, ncol = 3)  # ncol=3 arranges the plots in one row


```

The histograms **\[CROSS-REFERENCE HERE\]** reveal significant trends in sample sizes across the different ranges, indicating a general tendency towards smaller samples in the polling data. Most values within the 0-10,000 range cluster at the lower end, suggesting that many polls are conducted with limited respondent numbers, which may compromise the reliability and representativeness of the results. **However, this conclusion is very much dependent on what we determine to be an acceptable sample size for the results to be considered reliable**. In the 10,001-20,000 range, while there is some increase in sample sizes, the majority still falls within the 10,000-12,500 bracket, with few reaching the upper limits.

The findings from the 20,001-30,000 range further illustrate the scarcity of larger sample sizes, with only a couple of values noted at each of the significant points. This distribution emphasizes a preference for smaller samples in the polling data analyzed.

In the context of polling results, these findings suggest potential limitations in the accuracy and validity of the insights generated. Larger sample sizes are typically preferred in polling as they tend to yield more reliable estimates and reduce the margin of error, leading to a better representation of the population's views. Consequently, the predominance of smaller sample sizes in this data set may raise concerns about the robustness of the polling findings, potentially affecting their applicability in broader analyses or decision-making processes. **However, we must determine an acceptable and ideal sample size that would allow us to determine the reliability of these results.**

### 3) Average percentage of support for top candidates (pct): how do we display this, which kind of graph, plot?

### 4) Number of polls conducted by State: map showing different colours for the different states and the number of polls conducted for them. this will only include the 'state' polls then. not national polls.

### 5) population, type fo voters simple graph, chart: how many likely vs registered voters.

### 6) methodology: count for each type fo methodology and how many times it was used

## Graphs for Imp. Variables (relationships)

## Measurement

Some paragraphs about how we go from a phenomena in the world to an entry in the dataset.

Public support for political candidates is an essential part of understanding electoral dynamics for a variety of reasons. Public polls provide insights into the preferences and beliefs of certain groups of people, usually voters in specific states or districts, and sometimes to also correlate preferences to demographic information (race, age, neighbourhood, etc).

The results help inform policy decisions, help track the likely outcomes of elections, help government officials and candidates decide where to invest time, funds, and resources to get the desired results. Polling organizations aim to capture data by surveying a representative sample of voters, free from government intervention.

Polling organizations conduct these surveys through various methods. For our particular data set, the following methods have been used to gauge public sentiment:

-   App panel

-   Online panel

-   Email

-   Online ad

-   IVR (interactive voice response)

-   Live phone

-   Text

-   Text-to-web

-   Probability panel

-   Mail-to-phone

Each pollster chooses a method(s) based on what they find to be most convenient, efficient, and providing the best results. Moreover, there are some variables which affect the validity and reliability of the polling results, such as sample size, methodology, population type, etc. Larger samples generally lead to more reliable estimates of public opinion. Some methodologies might be considered more reliable due to certain factors. The pollsters also use different sampling methods (random, stratified, etc) which can affect the validity of the polling results as well.

After collecting the required data, the polling organizations transform these results into more easily comprehensible variables such as numeric grade, poll score, transparency score, and a percentage. These scores are calculated through an extensive process of calculating excess error and bias, adjusting for race difficulty, calculating predictive error and bias, calculating pollster transparency, and combining everything into one single â€˜POLLSCOREâ€™, while accounting for luck as well.

Variables such as pollster rating and transparency score help validate the reliability of the pollster and the polling results they publish.

Each entry in the data set represents a glimpse of public opinion at a specific time and location, capturing measured support percentages alongside contextual variables such as pollster, methodology, and sample size. This extensive approach allows for an enhanced understanding of public support dynamics, facilitating informed analyses of electoral outcomes.

# Polls of Polls to Forcast Winning Candidate

## Forecasting Approach

The polls of polls methodology is widely used in election prediction as it aggregates multiple polls to provide a more reliable estimate of voter support, rather than relying on any single poll. The goal is to reduce errors and biases present in individual polls by using a weighted average of many different polls. In our approach, we calculate the weights based on several important variables. Sample size is included because larger polls are generally more reliable and have smaller margins of error. Pollscore (or pollster rating) reflects the empirical quality of a pollster, balancing both accuracy and bias, allowing us to give more weight to those with a proven track record. We also account for the pollscope, differentiating between national and state-level polls, as state polls may offer more localized insights into voter sentiment. We also use recency as more recent polls give us a more accurate estimate of voter sentiment.

However, we acknowledge that even with these adjustments, there may still be underlying biases from individual pollsters toward the candidates. To account for this, we will select a baseline pollster with a strong rating, high transparency, and minimal historical error, and regress the support percentage (pct) on pollster, including the same variables as controls whcih was used in the weighting method, such as sample size, pollscore and pollscope.Additionally, we will control for the state of the poll, as support levels can vary regionally. Therefore each pollster's coefficient in the regression model will represent the bias of that pollster relative to the baseline pollster. Controlling for these factors is crucial because it ensures that the differences between pollsters are not driven by other factors, such as differences in sample size or the types of polls they conduct, but by their inherent bias in support estimates. Once we have calculated the weights, we will use them to generate a weighted average of support for Kamala Harris and Donald Trump, ultimately comparing their weighted support to forecast the likely winner of the election.

## **Exploration of Control Variables**

### Investigating the Effect of Pollscore on Candidate Support

```{r}

# Define pollscore ranges (you can adjust the binwidth if needed)
cleaned_data <- analysis_data %>%
  mutate(pollscore_range = cut(pollscore, breaks = seq(-1.5, 1, by = 0.3), include.lowest = TRUE))

# Calculate the average support for each candidate within each pollscore range
average_support <- cleaned_data %>%
  group_by(pollscore_range, candidate_name) %>%
  summarise(avg_support = mean(pct, na.rm = TRUE)) %>%
  ungroup()

# Create a bar plot
ggplot(average_support, aes(x = pollscore_range, y = avg_support, fill = candidate_name)) +
  geom_bar(stat = "identity", position = "dodge") +  # Dodge to separate bars for each candidate
  scale_fill_manual(values = c("Donald Trump" = "indianred", "Kamala Harris" = "steelblue")) +  # Custom colors
  labs(title = "Average Support for Candidates by Pollscore Range",
       x = "Pollscore Range",
       y = "Average Support (%)",
       fill = "Candidate") +
  theme_minimal() +  # Minimal theme
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5)
  )


```

### Investigating the Effect of Poll Recency on Candidate Support

```{r}
#| warning: false
#| message: false
#| echo: false
# Custom colors for candidates
trump_color <- "indianred"
harris_color <- "steelblue"

# Create the plot with the requested adjustments (recency vs support)
ggplot(analysis_data, aes(x = recency, y = pct, color = candidate_name)) +
  geom_point(size = 0.5, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) + # Smaller dots with slight transparency
  scale_color_manual(values = c("Donald Trump" = trump_color, "Kamala Harris" = harris_color)) +  # Custom colors
  theme_minimal() +  # Remove grey background
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.title = element_blank(),  # Remove "candidate_name" as title
    legend.position = "bottom",  # Move legend to the bottom
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine for Kamala Harris
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(title = "Recency vs Support for Kamala Harris and Donald Trump",
       x = "Poll Recency (days since poll end date)", 
       y = "Support (%)") +
  facet_wrap(~ candidate_name, scales = "free_y")  # Separate plots for each candidate

```

### Investigate the effect of sample size on support estimates

```{r}
#| warning: false
#| message: false
#| echo: false


# Custom colors for candidates
trump_color <- "indianred"
harris_color <- "steelblue"

# Create the plot with the requested adjustments
ggplot(analysis_data, aes(x = sample_size, y = pct, color = candidate_name)) +
  geom_point(size = 0.5, alpha = 0.9) + # Smaller dots with slight transparency
  geom_smooth(method = "lm", se = FALSE)+ 
 
  scale_color_manual(values = c("Donald Trump" = trump_color, "Kamala Harris" = harris_color)) +  # Custom colors
  theme_minimal() +  # Remove grey background
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.title = element_blank(),  # Remove "candidate_name" as title
    legend.position = "bottom",  # Move legend to the bottom
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine for Kamala Harris
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(title = "Sample Size vs Support for Kamala Harris and Donald Trump",
       x = "Sample Size", 
       y = "Support (%)") +
  facet_wrap(~ candidate_name, scales = "free_y")
```

### Distribution of support for Candiates by State

```{r}
# Calculate mean support for each candidate in each state
state_support <- analysis_data %>%
  group_by(state, candidate_name) %>%
  summarise(mean_support = mean(pct)) %>%
  ungroup()

# Pivot the data to have one row per state with separate columns for Harris and Trump
state_support_wide <- state_support %>%
  pivot_wider(names_from = candidate_name, values_from = mean_support)
```

```{r}
# Calculate mean support for each candidate in each state
state_support <- analysis_data %>%
  group_by(state, candidate_name) %>%
  summarise(mean_support = mean(pct))

# Calculate the proportion of support for Kamala Harris
state_support <- state_support %>%
  mutate(harris_proportion = Kamala_Harris / (Kamala_Harris + Donald_Trump))

```

# Model

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.

### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.

# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <-
  readRDS(file = here::here("models/first_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```

# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this.

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows...

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```

\newpage

# References
