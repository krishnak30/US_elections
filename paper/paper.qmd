---
title: Forcasting US Elections using Polls-of-Polls
subtitle: "My subtitle if needed"
author: 
  - Shamayla Durrin
  - Denise Chang 
  - Krishna Kumar
thanks: "Code and data are available at: [https://github.com/krishnak30/US_elections](https://github.com/krishnak30/US_elections)."

date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

#install.packages("psych")
#install.packages("plotly")
install.packages("stringr")
library(psych)
library(tidyverse)
library(arrow)
library(here)
library(ggplot2)
library(kableExtra)
library(sf)
library(tidyverse)
library(plotly)
library(RColorBrewer)
library(stringr)
library(modelsummary)
analysis_data <- read_parquet(here("data/02-analysis_data/analysis_data.parquet"))
mapping_data <- st_read(here("data/03-shapefiles_data/US"))
```

# Introduction

# Data

### Measurement

In the dataset of our analysis, the process of measurement begins by capturing real-world public opinion through surveys. Polling organizations transform this observed phenomenon into structured data by recording support percentages, pollster details, and methodological choices, thus turning public sentiment into quantifiable entries in the dataset.

Public opinion polls are essential for understanding voter preferences and electoral dynamics, providing data that informs policy decisions and campaign strategies. Our data set captures public support for candidates at specific times, along with information on methodologies, sample sizes, and pollster ratings.

Polling organizations use various methods, such as App panels, IVR (Interactive Voice Response), Live phone calls, Text-to-web systems etc. These methods can influence the validity and reliability of results, with larger sample sizes generally yielding more reliable estimates. Some methods, like live phone interviews, may be seen as more trustworthy than others.

Our data set also includes important measures of pollster performance, such as numeric grades, poll scores, and transparency scores. These metrics are calculated by considering bias, race difficulty, predictive error, and transparency, allowing us to assess the accuracy of each poll. The "pollscore," for instance, reflects a pollster’s past performance and reliability, adjusted for potential biases.

### Data Cleaning

The raw data for this project, sourced from FiveThirtyEight, [@FiveThirtyEight] underwent a series of cleaning steps to prepare it for analysis. Initially, duplicate rows were removed to ensure that only unique observations remained, facilitated by the janitor package \[\@janitor\]. A new binary variable, 'national', was created to indicate whether a poll was conducted at the national or state level. Missing values in the 'state' column were replaced with "Not Applicable," and numeric grades were evaluated to filter out low-quality pollsters, keeping only those with a numeric grade above 1. This cutoff was selected to retain mid to high-level pollsters for more reliable results. These steps were performed using functions from the dpylr package \[\@dplyr\]. Furthermore, dates were standardized and converted into a proper format for analysis using the lubridate package \[\@lubridate\]. Polls related to Kamala Harris were retained for further analysis, and percentage support values were transformed into actual numbers of supporters based on sample size. Additionally, pollster counts below five were excluded to focus on more reliable data sources. Polls regarding Kamala Harris were filtered to include only those conducted after her official candidacy announcement on July 21, 2024, ensuring the data reflects post-announcement public sentiment.The cleaned dataset was saved in Parquet format for efficient storage and retrieval, using the arrow package \[\@arrow\].

### Summary of Variables of Interest

```{r}

#| include: false
#| warning: false
#| message: false

library(dplyr)
library(modelsummary)
library(tools)  
library(kableExtra)

# Set options to use kableExtra for table formatting
options(modelsummary_factory_default = 'kableExtra')

# Create the summary statistics table for the specified numerical variables
summary_table <- analysis_data %>%
  select(numeric_grade, pollscore, sample_size, num_support) %>%
  # Remove underscores and capitalize the first letter of each word
  rename_with(~ toTitleCase(gsub("_", " ", .))) %>%
  datasummary_skim(
    fun_numeric = list(
      Mean = mean,
      SD = sd,
      Min. = min,
      Median = median,
      Max. = max
    )
  )

# Display the summary statistics table
print(summary_table)

#kable(summary_table, caption = "Summary of Variables of Interest") %>%
  kable_styling(full_width = FALSE)

```

The summary statistics **\[CROSS REFERENCE HERE\]** for the numeric variables provide significant insights into the polling data. The **numeric grade** has a mean of 2.3 and a standard deviation of 0.6, suggesting that most polls are graded around a middle value, though there is moderate variability. The range, with a minimum of 1.1 and a maximum of 3.0, indicates the presence of both low-quality and higher-quality polls within the dataset.

The **pollscore** averages at −0.5, with a standard deviation of 0.6. Since lower poll scores indicate more favorable outcomes for certain candidates or parties, this negative mean suggests that the polls are, on average, skewing positively towards their targets. The minimum of −1.5 and maximum of 1.7 further illustrate the wide distribution of poll results, highlighting the variability in outcomes across different polls.

Looking at the **sample size**, the mean of 1962.5, accompanied by a substantial standard deviation of 2608.0, reflects significant variability in respondent numbers. With a minimum sample size of 382 and a maximum of 12,084, it is evident that while many polls are conducted with relatively small sample sizes, a few achieve much larger respondent pools. This variability can enhance the reliability of findings, particularly for those polls with larger sample sizes.

Finally, the **number of supporters** reported has a mean of 949.2 and a standard deviation of 1290.0, indicating that while the average number of supporters is relatively high, there is considerable variability. The minimum value of 172 and maximum of 6042 underscores the diverse contexts in which these polls were conducted.

Overall, these summary statistics emphasize the variability and range present in the polling data, which may impact the reliability and interpretation of the findings. The mixture of low and high values in numeric grade, pollscore, and sample size points to the importance of careful analysis when drawing conclusions from these polling efforts.

## Variables

The data set contains a wide range of variables that are essential for understanding polling outcomes. The following are key variables:

-   **`poll_id`**: Unique identifier for each poll conducted.

-   **`pollster`**: The polling organization that conducted the poll (e.g., YouGov, TIPP).

-   **`numeric_grade`**: A numeric rating indicating the pollster’s quality or reliability (e.g., 1.8).

-   **`pollscore`**: A score representing the reliability of the pollster in terms of error and bias.

-   **`state`**: The state where the poll was conducted (or "Not Applicable" for national polls).

-   **`start_date` / `end_date`**: Dates during which the poll was conducted.

-   **`national`**: A binary variable which determines where a poll was national (1) or state (0)

-   **`sample_size`**: The number of respondents in the poll.

-   **`population`**: The group surveyed (e.g., likely voters or registered voters).

-   **`candidate_name`**: The name of the candidate being polled (e.g., Kamala Harris, Donald Trump).

-   **`pct`**: The percentage of support for the candidate in the poll.

-   **`methodology`**: The method used to conduct the poll (e.g. online panel)

-   **`num_support`**: The number of people supporting a candidate in a poll

## High Level Data Cleaning

blah blah blah

## Summary Statistics

### 1) Average Pollster Quality by Poll Type (pollscore vs state/national): mean, median, mode, min, max

```{r}
#| warning: false
#| message: false
#| echo: false


library(dplyr)

# Generate summary statistics for national and state polls
summary_stats <- analysis_data %>%
  filter(!is.na(pollscore)) %>%  # Exclude rows with missing pollscore

  group_by(national) %>%         # Group by the binary 'national' column (1 for national, 0 for state)

  group_by(national) %>%        # Group by poll_scope

  summarize(
    Count = n(),
    Min = round(min(pollscore), 2),
    `1st Qu.` = round(quantile(pollscore, 0.25), 2),
    Median = round(median(pollscore), 2),
    Mean = round(mean(pollscore), 2),
    `3rd Qu.` = round(quantile(pollscore, 0.75), 2),
    Max = round(max(pollscore), 2),
    .groups = 'drop'  # Drop grouping structure for cleaner output
  ) %>%
  # Rename 'national' to 'Poll Type' and replace values
  mutate(`Poll Type` = ifelse(national == 1, "National", "State")) %>%
  select(`Poll Type`, Count, Min, `1st Qu.`, Median, Mean, `3rd Qu.`, Max)  # Reorder columns

# Print the summary statistics
kable(summary_stats)


```

As shown above, the summary statistics table **\[CROSS-REFERENCE HERE\]** offers an overview of poll scores for national and state polls, with 427 and 696 entries, respectively. Both categories feature a minimum poll score of -1.5, which a high level of validity and reliability. The first quartile shows national polls at -0.9, while state polls are slightly lower at -1.1. Median scores are -0.3 for national polls and -0.4 for state polls, suggesting that half of the polls in both categories yield moderately negative scores, reinforcing a trend of favorable sentiment. The mean score is -0.39 for national polls, indicating a less reliable perception compared to the -0.54 mean for state polls. The third quartile for national and state polls (-0.3) indicates that the upper 25% of scores are slightly above neutral. The maximum score of 1.7 for national polls reflects a notable negative perception for at least one candidate, while the maximum for state polls is 0.6. Overall, these statistics illustrate a strong reliability of pollsters considering their errors and bias.

### 2) Distribution of Sample Sizes

```{r}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(dplyr)
library(gridExtra) 

# Adjust the sample size range categories
analysis_data <- analysis_data %>%
  mutate(sample_size_range = case_when(
    sample_size <= 6000 ~ "0-6,000",
    sample_size > 6000 & sample_size <= 13000 ~ "6,001-13,000",
    TRUE ~ "Above 13,000"  # Catch any values above 13,000
  ))

# Plot for 0-6,000
p1 <- ggplot(analysis_data %>% filter(sample_size <= 6000), aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "lightgreen", color = NA, alpha = 0.7) +
  labs(title = "Sample Sizes (0-6,000)",
       x = "Sample Size",
       y = "Count") +
  scale_x_continuous(limits = c(0, 6000)) +  # Set x limits for the first plot
  scale_y_continuous(limits = c(0, 500)) +  # Adjust y limits based on data
  theme_minimal() +
  theme(plot.title = element_text(size = 10, hjust = 0.5),  # **Center the title**
        axis.title.y = element_text(margin = margin(r = 15)))  # **Increased gap for y-axis label**

# Plot for 6,001 - 13,000
p2 <- ggplot(analysis_data %>% filter(sample_size > 6000 & sample_size <= 13000), aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "indianred", color = NA, alpha = 0.7) +
  labs(title = "Sample Sizes (6,001-13,000)",
       x = "Sample Size",
       y = "Count") +
  scale_x_continuous(limits = c(6001, 13000)) +  # Set x limits for the second plot
  scale_y_continuous(limits = c(0, 20)) +  # Adjust y limits based on data
  theme_minimal() +
  theme(plot.title = element_text(size = 10, hjust = 0.5),  # **Center the title**
        axis.title.y = element_text(margin = margin(r = 10)))  # **Increased gap for y-axis label**

# Arrange the two plots side by side
grid.arrange(p1, p2, ncol = 2)  # ncol=2 arranges the plots in one row


```

The histograms **\[CROSS-REFERENCE HERE\]** reveal significant trends in sample sizes across the different ranges, indicating a general tendency towards smaller samples in the polling data. **In Plot 1**, most of the values are concentrated on the left side, with very few values extending beyond the 4,500 mark. This clustering suggests that many polls are conducted with limited respondent numbers, which may compromise the reliability and representativeness of the results.

In **Plot 2**, the values are somewhat evenly divided between 6,000 and the maximum sample size of 12,084. This distribution shows a broader representation of sample sizes, indicating that while there are still smaller samples, there are also a notable number of polls that feature larger sample sizes. This contrast with Plot 1 highlights a shift towards more robust polling efforts.

The findings from these histograms suggest potential limitations and advantages in the accuracy and validity of the insights generated from the polling data. Smaller sample sizes are typically less desirable in polling, as they can yield less reliable estimates and increase the margin of error. Consequently, the predominance of smaller sample sizes in Plot 1 raises concerns about the robustness of the polling findings. However, the more even distribution in Plot 2 suggests that some polls conducted with larger sample sizes could enhance the reliability of the results.

Ultimately, the analysis emphasizes the need to consider sample sizes critically when interpreting polling data, as larger sample sizes tend to provide a better representation of the population's views. Thus, while many polls may fall short of ideal sample sizes, the existence of larger samples in the data set can offer valuable insights into public opinion. **However, we need to determine an ideal and acceptable sample size that would be enough for the polling results to be considered reliable, before we conclude smaller samples as being unreliable.**

### 3) Top 10 Pollsters and their Average Pollscores and Numeric Grades

```{r}
library(dplyr)

# Get the top 10 pollsters by frequency along with their average pollscore and numeric grade
top_pollsters <- analysis_data %>%
  group_by(pollster) %>%           # Group by the 'pollster' variable
  summarize(
    Count = n(),                   # Count the number of occurrences
    Average_Pollscore = round(mean(pollscore, na.rm = TRUE), 2),  # Average pollscore
    Average_Numeric_Grade = round(mean(numeric_grade, na.rm = TRUE), 2),  # Average numeric grade
    .groups = 'drop'               # Drop grouping structure for cleaner output
  ) %>%
  arrange(desc(Count)) %>%         # Arrange in descending order of count
  slice_head(n = 10)                # Select the top 10 pollsters

# Rename the columns to remove underscores and capitalize 'Pollster'
colnames(top_pollsters) <- c("Pollster", "Count", "Average Pollscore", "Average Numeric Grade")

kable(top_pollsters, format = "html", caption = "Top 10 Pollsters by Frequency")

```

The examination of the top 10 pollsters in **\[CROSS REFERENCE HERE\]** highlights their significance in electoral forecasting. **Morning Consult**, with the highest volume of polls (235), shows a slightly optimistic average pollscore of −0.3, indicating some reliability and validity. Conversely, **Siena/NYT** stands out with the lowest average pollscore of −1.5 and the highest numeric grade of 3.0, suggesting a rigorous methodology that enhances its credibility in predicting electoral outcomes.

In contrast, **Redfield & Wilton Strategies** displays an average pollscore of 0.4, which may create a misleadingly pessimistic view of voter sentiment. This underscores the necessity of considering multiple polling sources for a nuanced understanding of electoral dynamics.

The presence of several pollsters with negative scores indicates a general trend towards specific candidates, which is crucial for shaping campaign strategies. Overall, these findings reinforce the critical role of trustworthy polling in influencing public perception and political strategy during elections.

### 5) Histogram of End Dates

yet to do.

## Measurement

Some paragraphs about how we go from a phenomena in the world to an entry in the dataset.

Public support for political candidates is an essential part of understanding electoral dynamics for a variety of reasons. Public polls provide insights into the preferences and beliefs of certain groups of people, usually voters in specific states or districts, and sometimes to also correlate preferences to demographic information (race, age, neighborhood, etc).

The results help inform policy decisions, help track the likely outcomes of elections, help government officials and candidates decide where to invest time, funds, and resources to get the desired results. Polling organizations aim to capture data by surveying a representative sample of voters, free from government intervention.

Polling organizations conduct these surveys through various methods. For our particular data set, the following methods have been used to gauge public sentiment:

-   App panel

-   Online panel

-   Email

-   Online ad

-   IVR (interactive voice response)

-   Live phone

-   Text

-   Text-to-web

-   Probability panel

-   Mail-to-phone

Each pollster chooses a method(s) based on what they find to be most convenient, efficient, and providing the best results. Moreover, there are some variables which affect the validity and reliability of the polling results, such as sample size, methodology, population type, etc. Larger samples generally lead to more reliable estimates of public opinion. Some methodologies might be considered more reliable due to certain factors. The pollsters also use different sampling methods (random, stratified, etc) which can affect the validity of the polling results as well.

After collecting the required data, the polling organizations transform these results into more easily comprehensible variables such as numeric grade, poll score, transparency score, and a percentage. These scores are calculated through an extensive process of calculating excess error and bias, adjusting for race difficulty, calculating predictive error and bias, calculating pollster transparency, and combining everything into one single ‘POLLSCORE’, while accounting for luck as well.

Variables such as pollster rating and transparency score help validate the reliability of the pollster and the polling results they publish.

Each entry in the data set represents a glimpse of public opinion at a specific time and location, capturing measured support percentages alongside contextual variables such as pollster, methodology, and sample size. This extensive approach allows for an enhanced understanding of public support dynamics, facilitating informed analyses of electoral outcomes.

# Polls of Polls to Forecast Winning Candidate

## Forecasting Approach

The polls of polls methodology is widely used in election prediction as it aggregates multiple polls to provide a more reliable estimate of voter support, rather than relying on any single poll. The goal is to reduce errors and biases present in individual polls by using a weighted average of many different polls.

In our approach, we will employ linear modeling of voter support percentage (pct) on pollster and other independent variables such as sample size, poll recency, and poll scope (state vs. national). This will allow us to smooth out the inherent noise, biases, and variability across different pollsters. Once we obtain the predicted values from our model, we will weight these predictions based on the numeric grade (quality score) of each pollster to calculate an overall national estimate of the outcome. Additionally, we will separately compute estimates for key battleground states, where voter behavior can be more volatile and pivotal in deciding the final outcome of the election. This approach helps us capture both national trends and critical state-level dynamics.

## **Exploration of Independent Variables**

### Support for Kamala Harris by Sample Size and Polling Level

\[\@fig-sample\] suggests that the relationship between support for Kamala Harris and sample size shows a slight positive trend, indicating a small increase in support as sample sizes grow. Both national (blue) and state (green) polls reflect this upward trend, although the effect is minimal. Since voting outcomes often depend on small margins, this subtle increase in support with larger sample sizes could still be relevant in predicting overall support.

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: fig-sample
#| fig-cap: Sample Size vs. Support for Kamala Harris, Colored by Pollscope (National vs. State)

# Custom colors for pollscope
national_color <- "chartreuse3"
state_color <- "steelblue"

# Create the plot for Kamala Harris with the requested adjustments
ggplot(analysis_data %>% filter(candidate_name == "Kamala Harris"), 
       aes(x = sample_size, y = pct, color = factor(national))) +
  geom_point(size = 0.5, alpha = 0.9) +  # Smaller dots with slight transparency
  geom_smooth(aes(group = 1), method = "lm", se = FALSE, color = "azure4") +  # Linear model smoothing
  
  # Custom colors for national (1 for National, 0 for State)
  scale_color_manual(values = c("1" = national_color, "0" = state_color), 
                     labels = c("National", "State")) +  # Pollscope categories with custom colors
  
  # Same minimal styling
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.title = element_blank(),  # No title for the legend
    legend.position = "bottom",  # Move legend to the bottom
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(x = "Sample Size", y = "Support (%)")  # No title, just axis labels

```

### Variation in Support for Kamala Harris Among Most Frequent Pollsters

[@fig-top5pollsters] above shows the variation in support for Kamala Harris across the five most frequent pollsters. Notably, the median support levels differ between pollsters, indicating variability in central estimates of Harris’s support. For instance, Siena/NYT reports a higher median compared to Redfield & Wilton Strategies. Additionally, the differing ranges of support estimates highlight potential discrepancies in the methodologies, sample sizes, and biases employed by each pollster. This suggests that different pollster have different interpretations of candidate support, underlining the importance of aggregating data from multiple sources to avoid over-reliance on any single pollster.

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: fig-top5pollsters
#| fig-cap: "Support for Kamala Harris across the top 5 pollsters by poll count."

# Get top 5 pollsters by poll count
top5_pollsters <- analysis_data %>%
  filter(candidate_name == "Kamala Harris") %>%
  count(pollster, sort = TRUE) %>%
  top_n(5, n) %>%
  pull(pollster)

# Filter the data for only these top 5 pollsters
top5_data <- analysis_data %>%
  filter(pollster %in% top5_pollsters & candidate_name == "Kamala Harris")

# Custom colors for boxplot fill
boxplot_colors <- c("chartreuse3", "steelblue", "indianred", "purple", "darkorange")

# Create the boxplots with thinner borders, smaller points, and rotated x-axis labels
ggplot(top5_data, aes(x = pollster, y = pct, fill = pollster)) +
  geom_boxplot(size = 0.2, outlier.size = 0.5) +  # Thinner box lines and smaller outliers
  scale_fill_manual(values = boxplot_colors) +
  theme_minimal() +  # Minimal theme
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.position = "none",  # No legend needed for boxplots
    axis.line = element_line(color = "black"),  # Add axis lines
    plot.title = element_text(hjust = 0.5),  # Center title
    axis.text.x = element_text(angle = 20, hjust = 1),  # Rotate x-axis labels for better readability
    axis.line.y.right = element_line(color = "black"),  # Add right spine for clean look
    axis.line.x.top = element_blank(),  # Remove top spine
    panel.border = element_blank()  # Remove panel border
  ) +
  labs(x = "Pollster", y = "Support (%)")  # Labels

```

### Support for Kamala Harris Based on Recency of Polls and Pollster Quality

In \[\@fig-enddate\], we observe a slight increasing trend in support for Kamala Harris as the end date of polls progresses towards October. This may indicate a gradual improvement in her polling performance over time. The color gradient, reflecting the pollster's error and bias (Pollscore), reveals variations in bias: greener points represent more negative biases, while redder points indicate more positive biases.

While the recency of the polls shows a stable trend in Harris's support, the wide distribution of bias (Pollscore) values suggests that pollster quality can vary substantially. These variations could influence the model's predictions if left unaddressed. Hence, including both recency and pollster quality as predictor variables in the model might be valuable to account for such variations. Further model diagnostics and validation will help decide if these variables are significant.

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: fig-enddate
#| fig-cap: "Support for Kamala Harris based on End Date and colored by Pollster's Error and Bias."

# Define the color gradient based on pollscore
ggplot(analysis_data %>% filter(candidate_name == "Kamala Harris"), aes(x = end_date, y = pct, color = pollscore)) +
  geom_point(size = 0.8, alpha = 1) +  # Adjust point size and transparency
  geom_smooth(method = "lm", se = FALSE, color = "azure4") +  # Add a smooth trend line for overall support
  scale_color_gradient2(low = "blue", mid = "green", high = "red", midpoint = 0) +  # Color gradient
  theme_minimal() +  # Minimal theme as usual
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.position = "bottom",  # Legend at the bottom
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine for clean look
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(x = "Date Poll Ended",
       y = "Support (%)",
       color = "Error and Bias of Pollster (Low to High)")  # Updated label for color gradient

```

### Political Leanings of Different State

[@fig-mapinterac] provides a state-level representation of Kamala Harris's average support across the U.S., with the darker blue shades indicating states where her support is higher, and the redder tones showing where her support is lower. Gray areas represent states where we have no polling data available.

The gradient highlights notable geographic trends. Harris has the strongest support in states like Vermont, Massachusetts, and New York, while states like Utah and West Virginia show considerably lower levels of support. A distinct pattern emerges in the West, where coastal states like California favor Harris more strongly, contrasting with inland states like Wyoming and Utah.

This reveals that political leanings differ greatly by state, and this regional variation should be controlled for in any regression model analyzing poll data. The map serves as a useful tool for visualizing regional variations in voter support for Kamala Harris as reflected in the available polling data.

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: fig-mapinterac
#| fig-cap: "Average Support for Kamala Harris in Different States estimated by Polls"
  # For string manipulation

# Filter out non-continental states (Hawaii and Alaska)
us_states_filtered <- us_states %>% 
  filter(!NAME %in% c("Alaska", "Hawaii"))

# Ensure correct merge between map data and Kamala's support data
map_data_full <- left_join(us_states_filtered, mapping_data, by = c("NAME" = "state"))

# Create centroids for state labels
state_centroids <- st_centroid(us_states_filtered)

# Plot using geom_sf() for sf objects
p <- ggplot(map_data_full) +
  geom_sf(aes(fill = mean_support_kamala, text = paste("State:", str_to_title(NAME), "<br>Average Support:", round(mean_support_kamala, 2))), color = "white") +  # Capitalize state names
  scale_fill_distiller(palette = "RdBu", direction = 1, na.value = "gray", name = "Kamala's Support") +  # Reverse RdBu color gradient
  geom_text(data = state_centroids, aes(geometry = geometry, label = STUSPS), stat = "sf_coordinates", size = 2) +  # State labels
  labs(title = "Support for Kamala Harris by State", 
       caption = "States with no polling data are shown in gray") +
  theme_minimal() +
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5),
    panel.grid = element_blank(),  # Remove grid lines
    axis.title = element_blank(),  # No axis titles for maps
    axis.text = element_blank(),   # No axis labels
    axis.ticks = element_blank()   # No axis ticks
  )



```

### Investigating the Effect of Poll Recency on Candidate Support

```{r}
#| warning: false
#| message: false
#| echo: false
# Custom colors for candidates
trump_color <- "indianred"
harris_color <- "steelblue"

# Create the plot with the requested adjustments (recency vs support)
ggplot(analysis_data, aes(x = recency, y = pct, color = candidate_name)) +
  geom_point(size = 0.5, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) + # Smaller dots with slight transparency
  scale_color_manual(values = c("Donald Trump" = trump_color, "Kamala Harris" = harris_color)) +  # Custom colors
  theme_minimal() +  # Remove grey background
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.title = element_blank(),  # Remove "candidate_name" as title
    legend.position = "bottom",  # Move legend to the bottom
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine for Kamala Harris
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(title = "Recency vs Support for Kamala Harris and Donald Trump",
       x = "Poll Recency (days since poll end date)", 
       y = "Support (%)") +
  facet_wrap(~ candidate_name, scales = "free_y")  # Separate plots for each candidate

```

### Investigate the effect of sample size on support estimates

```{r}
#| warning: false
#| message: false
#| echo: false


# Custom colors for candidates
trump_color <- "indianred"
harris_color <- "steelblue"

# Create the plot with the requested adjustments
ggplot(analysis_data, aes(x = sample_size, y = pct, color = candidate_name)) +
  geom_point(size = 0.5, alpha = 0.9) + # Smaller dots with slight transparency
  geom_smooth(method = "lm", se = FALSE)+ 
 
  scale_color_manual(values = c("Donald Trump" = trump_color, "Kamala Harris" = harris_color)) +  # Custom colors
  theme_minimal() +  # Remove grey background
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.title = element_blank(),  # Remove "candidate_name" as title
    legend.position = "bottom",  # Move legend to the bottom
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine for Kamala Harris
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(title = "Sample Size vs Support for Kamala Harris and Donald Trump",
       x = "Sample Size", 
       y = "Support (%)") +
  facet_wrap(~ candidate_name, scales = "free_y")
```

### Distribution of support for Candidates by State

```{r}
# Calculate mean support for each candidate in each state
state_support <- analysis_data %>%
  group_by(state, candidate_name) %>%
  summarise(mean_support = mean(pct)) %>%
  ungroup()

# Pivot the data to have one row per state with separate columns for Harris and Trump
state_support_wide <- state_support %>%
  pivot_wider(names_from = candidate_name, values_from = mean_support)
```

```{r}
# Calculate mean support for each candidate in each state
state_support <- analysis_data %>%
  group_by(state, candidate_name) %>%
  summarise(mean_support = mean(pct))

# Calculate the proportion of support for Kamala Harris
state_support <- state_support %>%
  mutate(harris_proportion = Kamala_Harris / (Kamala_Harris + Donald_Trump))

# Turn ggplot to plotly for interactivity, add hover template for better control over display
ggplotly(p, tooltip = "text")


```

# Model

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.

### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.

# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <-
  readRDS(file = here::here("models/first_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```

# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this.

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows...

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```

\newpage

# References
