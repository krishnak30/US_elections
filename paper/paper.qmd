---
title: Forcasting US Elections using Polls-of-Polls
subtitle: "My subtitle if needed"
author: 
  - Shamayla Durrin
  - Denise Chang 
  - Krishna Kumar
thanks: "Code and data are available at: [https://github.com/krishnak30/US_elections](https://github.com/krishnak30/US_elections)."
date: today
date-format: long
abstract: "This paper uses a polls of polls approach to predict voter support for Kamala Harris in the 2024 U.S. election, aggregating multiple polls to reduce individual bias and improve accuracy. Through a regression model incorporating factors such as pollster, sample size, state, and recency, we predict her overall support to be 47.78%, just below the 50% threshold. Our analysis highlights the importance of poll quality and recency in understanding trends in public opinion. These findings are significant as they provide a more reliable estimate of electoral outcomes, helping to inform political strategies and public discourse."
format: html
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(psych)
library(tidyverse)
library(arrow)
library(here)
library(ggplot2)
library(kableExtra)
library(sf)
library(tidyverse)
library(plotly)
library(RColorBrewer)
library(stringr)
library(modelsummary)
library(broom)
library(sf)
analysis_data <- read_parquet(here("data/02-analysis_data/analysis_data.parquet"))
mapping_data <- st_read(here("data/03-mapping_data/US"))
mean_support_data <- read_parquet(here("data/02-analysis_data/map_data.parquet"))
```

# Introduction

While individual polls provide snapshots of public opinion, they are often subject to biases and methodological differences. To address these limitations, this paper uses a "polls of polls" approach to predict support for Kamala Harris, aggregating multiple polls to smooth out inconsistencies. By combining data from various polling organizations, our aim is to create a more accurate forecast of her voter support leading up to the 2024 U.S. election.

We employ a linear regression model to estimate Kamala Harris' support percentage, incorporating key variables such as pollster, sample size, state, and recency. To account for the varying reliability of different polls, we weight predictions using each pollster’s numeric grade. Our model also explores the role of recency, with recent polls showing a decline in support for Harris. The final weighted prediction for her support stands at 47.78%, indicating that her overall backing falls just below the 50% threshold, suggesting potential challenges in gaining majority support.

This paper contributes to the field of election forecasting by emphasizing the importance of poll quality and recency in prediction models. The structure of the paper proceeds as follows: we first introduce the dataset and methodology, followed by model selection and results. We then discuss the implications of our findings, and conclude with limitations and potential directions for future research.

# Data

### Measurement

In the dataset of our analysis, the process of measurement begins by capturing real-world public opinion through surveys. Polling organizations transform this observed phenomenon into structured data by recording support percentages, pollster details, and methodological choices, thus turning public sentiment into quantifiable entries in the dataset.

Public opinion polls are essential for understanding voter preferences and electoral dynamics, providing data that informs policy decisions and campaign strategies. Our data set captures public support for candidates at specific times, along with information on methodologies, sample sizes, and pollster ratings.

Polling organizations use various methods, such as App panels, IVR (Interactive Voice Response), Live phone calls, Text-to-web systems etc. These methods can influence the validity and reliability of results, with larger sample sizes generally yielding more reliable estimates. Some methods, like live phone interviews, may be seen as more trustworthy than others.

The dataset also includes important measures of pollster performance, such as numeric grades, poll scores, and transparency scores. These metrics are calculated by considering bias, race difficulty, predictive error, and transparency, allowing us to assess the accuracy of each poll. The "pollscore," for instance, reflects a pollster’s past performance and reliability, adjusted for potential biases.

### Data Cleaning

The raw data for this project, sourced from FiveThirtyEight, [@FiveThirtyEight] underwent a series of cleaning steps to prepare it for analysis. Initially, duplicate rows were removed to ensure that only unique observations remained, facilitated by the janitor package \[\@janitor\]. A new binary variable, 'national', was created to indicate whether a poll was conducted at the national or state level. Missing values in the 'state' column were replaced with "Not Applicable," and numeric grades were evaluated to filter out low-quality pollsters, keeping only those with a numeric grade above 1. This cutoff was selected to retain mid to high-level pollsters for more reliable results. These steps were performed using functions from the dpylr package \[\@dplyr\]. Furthermore, dates were standardized and converted into a proper format for analysis using the lubridate package \[\@lubridate\]. Polls related to Kamala Harris were retained for further analysis, and percentage support values were transformed into actual numbers of supporters based on sample size. Additionally, pollster counts below five were excluded to focus on more reliable data sources. Polls regarding Kamala Harris were filtered to include only those conducted after her official candidacy announcement on July 21, 2024, ensuring the data reflects post-announcement public sentiment.The cleaned dataset was saved in Parquet format for efficient storage and retrieval, using the arrow package \[\@arrow\].

### Summary Statistics of Variables of Interest

```{r}
#| include: false
#| warning: false
#| message: false
#| echo: false 

library(dplyr)
library(modelsummary)
library(tools)  
library(kableExtra)

# Set options to use kableExtra for table formatting
options(modelsummary_factory_default = 'kableExtra')

# Create the summary statistics table for the specified numerical variables
summary_table <- analysis_data %>%
  select(numeric_grade, pollscore, sample_size, num_support) %>%
  # Remove underscores and capitalize the first letter of each word
  rename_with(~ toTitleCase(gsub("_", " ", .))) %>%
  datasummary_skim(
    fun_numeric = list(
      Mean = mean,
      SD = sd,
      Min. = min,
      Median = median,
      Max. = max
    )
  )


print(summary_table)

#(summary_table)

#kable(summary_table, caption = "Summary of Variables of Interest") %>% kable_styling(full_width = FALSE)

```

The summary statistics table presents key descriptive measures for the numeric variables of interest. The Numeric Grade variable has a mean of 2.3, with values ranging from a minimum of 1.1 to a maximum of 3.0. It has a standard deviation of 0.6, indicating relatively low variability. The Pollscore variable averages -0.5, with a standard deviation of 0.6 and a range between -1.5 and 1.7. The Sample Size has a wider range, with an average of 1962.5 and a standard deviation of 2608.0, indicating significant variation across polls, with values spanning from 382 to 12084. Lastly, Num Support, calculated as the number of respondents supporting Kamala Harris, shows a mean of 949.2, with values ranging from 172 to 6042 and a standard deviation of 1290.0, highlighting substantial differences across the polls.

### Distribution of Sample Size of Polls

```{r}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)

# Plot histogram for all sample sizes
ggplot(analysis_data, aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "lightblue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Sample Sizes",
       x = "Sample Size",
       y = "Count") +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),  # Remove grid lines
    plot.title = element_text(size = 14, hjust = 0.5),  # Center the title
    axis.title.y = element_text(margin = margin(r = 10))  # Increased gap for y-axis label
  )

```

The histogram of sample sizes reveals a clear right-skewed distribution. Most of the sample sizes are clustered between 0 and 3000, with a sharp peak around 1000-1500. This indicates that the majority of polls are conducted with smaller sample sizes. As sample size increases, the frequency significantly drops, with very few polls conducted with sample sizes larger than 5000, though there are a few outliers with sizes approaching 10,000 or more. This wide range in sample sizes might affect the precision of estimates in different polls.

### Most Frequent Pollsters

```{r}
#| warning: false
#| message: false
#| echo: false
# Get the top 10 pollsters by frequency along with their average pollscore and numeric grade
top_pollsters <- analysis_data %>%
  group_by(pollster) %>%           # Group by the 'pollster' variable
  summarize(
    Count = n(),                   # Count the number of occurrences
    Average_Pollscore = round(mean(pollscore, na.rm = TRUE), 2),  # Average pollscore
    Average_Numeric_Grade = round(mean(numeric_grade, na.rm = TRUE), 2),  # Average numeric grade
    .groups = 'drop'               # Drop grouping structure for cleaner output
  ) %>%
  arrange(desc(Count)) %>%         # Arrange in descending order of count
  slice_head(n = 10)                # Select the top 10 pollsters

# Rename the columns to remove underscores and capitalize 'Pollster'
colnames(top_pollsters) <- c("Pollster", "Count", "Average Pollscore", "Average Numeric Grade")

kable(top_pollsters, format = "markdown", caption = "Top 10 Pollsters by Frequency")

```

This table lists the top 10 pollsters based on the frequency of polls they conducted. Morning Consult leads with 235 polls, followed by Siena/NYT and Redfield & Wilton Strategies with 94 and 88 polls, respectively. The table also shows each pollster's average poll score and average numeric grade. Most pollsters have negative poll scores, indicating a lower perceived reliability, except for Redfield & Wilton Strategies. However, the numeric grades vary, with Siena/NYT, YouGov, and Emerson receiving relatively high grades around 3, while others like TIPP and Redfield & Wilton Strategies are rated slightly lower.

# Forcasting Election Outcome through Pooling Polls

## Forecasting Approach

The polls of polls methodology is widely used in election prediction as it aggregates multiple polls to provide a more reliable estimate of voter support, rather than relying on any single poll. The goal is to reduce errors and biases present in individual polls by using a weighted average of many different polls.

In our approach, we will employ linear modeling of voter support percentage (pct) on pollster and other independent variables such as sample size, poll recency, and poll scope (state vs. national). This will allow us to smooth out the inherent noise, biases, and variability across different pollsters. Once we obtain the predicted values from our model, we will weight these predictions based on the numeric grade (quality score) of each pollster to calculate an overall national estimate of the outcome. Additionally, we will separately compute estimates for key battleground states, where voter behavior can be more volatile and pivotal in deciding the final outcome of the election. This approach helps us capture both national trends and critical state-level dynamics.

## **Exploration of Independent Variables**

### Support for Kamala Harris by Sample Size and Polling Level

\[\@fig-sample\] suggests that the relationship between support for Kamala Harris and sample size shows a slight positive trend, indicating a small increase in support as sample sizes grow. Both national (blue) and state (green) polls reflect this upward trend, although the effect is minimal. Since voting outcomes often depend on small margins, this subtle increase in support with larger sample sizes could still be relevant in predicting overall support.

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: fig-sample
#| fig-cap: Sample Size vs. Support for Kamala Harris, Colored by Pollscope (National vs. State)

# Custom colors for pollscope
national_color <- "chartreuse3"
state_color <- "steelblue"

# Create the plot for Kamala Harris with the requested adjustments
ggplot(analysis_data %>% filter(candidate_name == "Kamala Harris"), 
       aes(x = sample_size, y = pct, color = factor(national))) +
  geom_point(size = 0.5, alpha = 0.9) +  # Smaller dots with slight transparency
  geom_smooth(aes(group = 1), method = "lm", se = FALSE, color = "azure4") +  # Linear model smoothing
  
  # Custom colors for national (1 for National, 0 for State)
  scale_color_manual(values = c("1" = national_color, "0" = state_color), 
                     labels = c("National", "State")) +  # Pollscope categories with custom colors
  
  # Same minimal styling
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.title = element_blank(),  # No title for the legend
    legend.position = "bottom",  # Move legend to the bottom
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(x = "Sample Size", y = "Support (%)")  # No title, just axis labels

```

### Variation in Support for Kamala Harris Among Most Frequent Pollsters

[@fig-top5pollsters] above shows the variation in support for Kamala Harris across the five most frequent pollsters. Notably, the median support levels differ between pollsters, indicating variability in central estimates of Harris’s support. For instance, Siena/NYT reports a higher median compared to Redfield & Wilton Strategies. Additionally, the differing ranges of support estimates highlight potential discrepancies in the methodologies, sample sizes, and biases employed by each pollster. This suggests that different pollster have different interpretations of candidate support, underlining the importance of aggregating data from multiple sources to avoid over-reliance on any single pollster.

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: fig-top5pollsters
#| fig-cap: "Support for Kamala Harris across the top 5 pollsters by poll count."

# Get top 5 pollsters by poll count
top5_pollsters <- analysis_data %>%
  filter(candidate_name == "Kamala Harris") %>%
  count(pollster, sort = TRUE) %>%
  top_n(5, n) %>%
  pull(pollster)

# Filter the data for only these top 5 pollsters
top5_data <- analysis_data %>%
  filter(pollster %in% top5_pollsters & candidate_name == "Kamala Harris")

# Custom colors for boxplot fill
boxplot_colors <- c("chartreuse3", "steelblue", "indianred", "purple", "darkorange")

# Create the boxplots with thinner borders, smaller points, and rotated x-axis labels
ggplot(top5_data, aes(x = pollster, y = pct, fill = pollster)) +
  geom_boxplot(size = 0.2, outlier.size = 0.5) +  # Thinner box lines and smaller outliers
  scale_fill_manual(values = boxplot_colors) +
  theme_minimal() +  # Minimal theme
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.position = "none",  # No legend needed for boxplots
    axis.line = element_line(color = "black"),  # Add axis lines
    plot.title = element_text(hjust = 0.5),  # Center title
    axis.text.x = element_text(angle = 20, hjust = 1),  # Rotate x-axis labels for better readability
    axis.line.y.right = element_line(color = "black"),  # Add right spine for clean look
    axis.line.x.top = element_blank(),  # Remove top spine
    panel.border = element_blank()  # Remove panel border
  ) +
  labs(x = "Pollster", y = "Support (%)")  # Labels

```

### Support for Kamala Harris Based on Recency of Polls and Pollster Quality

In \[\@fig-enddate\], we observe a slight increasing trend in support for Kamala Harris as the end date of polls progresses towards October. This may indicate a gradual improvement in her polling performance over time. The color gradient, reflecting the pollster's error and bias (Pollscore), reveals variations in bias: greener points represent more negative biases, while redder points indicate more positive biases.

While the recency of the polls shows a stable trend in Harris's support, the wide distribution of bias (Pollscore) values suggests that pollster quality can vary substantially. These variations could influence the model's predictions if left unaddressed. Hence, including both recency and pollster quality as predictor variables in the model might be valuable to account for such variations. Further model diagnostics and validation will help decide if these variables are significant.

```{r}
#| warning: false
#| message: false
#| echo: false
#| label: fig-mapinterac
#| fig-cap: "Average Support for Kamala Harris in Different States estimated by Polls"

# Define the color gradient based on pollscore
ggplot(analysis_data %>% filter(candidate_name == "Kamala Harris"), aes(x = end_date, y = pct, color = pollscore)) +
  geom_point(size = 0.8, alpha = 1) +  # Adjust point size and transparency
  geom_smooth(method = "lm", se = FALSE, color = "azure4") +  # Add a smooth trend line for overall support
  scale_color_gradient2(low = "blue", mid = "green", high = "red", midpoint = 0) +  # Color gradient
  theme_minimal() +  # Minimal theme as usual
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.position = "bottom",  # Legend at the bottom
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine for clean look
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(x = "Date Poll Ended",
       y = "Support (%)",
       color = "Error and Bias of Pollster (Low to High)")  # Updated label for color gradient

```

### Political Leanings of Different State

[@fig-mapinterac] provides a state-level representation of Kamala Harris's average support across the U.S., with the darker blue shades indicating states where her support is higher, and the redder tones showing where her support is lower. Gray areas represent states where we have no polling data available.

The gradient highlights notable geographic trends. Harris has the strongest support in states like Vermont, Massachusetts, and New York, while states like Utah and West Virginia show considerably lower levels of support. A distinct pattern emerges in the West, where coastal states like California favor Harris more strongly, contrasting with inland states like Wyoming and Utah.

This reveals that political leanings differ greatly by state, and this regional variation should be controlled for in any regression model analyzing poll data. The map serves as a useful tool for visualizing regional variations in voter support for Kamala Harris as reflected in the available polling data.

```{r}
#| warning: false
#| message: false
#| echo: false
# Filter out non-continental states (Hawaii and Alaska)
us_states_filtered <- mapping_data %>% 
  filter(!NAME %in% c("Alaska", "Hawaii"))

# Ensure correct merge between map data and Kamala's support data
map_data_full <- left_join(us_states_filtered, mean_support_data, by = c("NAME" = "state"))

# Create centroids for state labels
state_centroids <- st_centroid(us_states_filtered)

# Plot using geom_sf() for sf objects
p <- ggplot(map_data_full) +
  geom_sf(aes(fill = mean_support, text = paste("State:", str_to_title(NAME), "<br>Average Support:", round(mean_support, 2))), color = "white") +  # Capitalize state names
  scale_fill_distiller(palette = "RdBu", direction = 1, na.value = "gray", name = "Kamala's Support") +  # Reverse RdBu color gradient
  geom_text(data = state_centroids, aes(geometry = geometry, label = STUSPS), stat = "sf_coordinates", size = 2) +  # State labels
  labs(title = "Support for Kamala Harris by State", 
       caption = "States with no polling data are shown in gray") +
  theme_minimal() +
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5),
    panel.grid = element_blank(),  # Remove grid lines
    axis.title = element_blank(),  # No axis titles for maps
    axis.text = element_blank(),   # No axis labels
    axis.ticks = element_blank()   # No axis ticks
  )

# Turn ggplot to plotly for interactivity, add hover template for better control over display
ggplotly(p, tooltip = "text")

```

# Model

In this section, we aim to address the inherent biases and differences present in various polling data to arrive at a robust prediction model. The core challenge lies in selecting a model with an optimal balance between complexity and fit, ensuring it accurately captures the dynamics of polling data while avoiding overfitting. To this end, we carefully evaluated different model specifications to determine the most appropriate one for our forecasting purpose.

Given that variables like numeric grade and pollscore are perfectly collinear with pollster, they were excluded from the regression analysis to avoid multicollinearity issues. These variables, however, remain integral to our weighting strategy, where they will be used to adjust for differences in polling accuracy and reliability. Instead, we focus on key features such as pollster, sample size, state, and recency, gradually adding complexity to the model.

By systematically comparing model specifications that incorporate these variables, we aim to select the model with the right balance between predictive accuracy and generalizability, ultimately providing the best possible forecast.

## Model Set Up

We aim to model the percentage of support for Kamala Harris in each poll as a function of the pollster the sample size, the state, and the recency of the poll.

$$
y_i = \alpha + \beta_1 \cdot \mathrm{pollster}_i + \beta_2 \cdot \mathrm{sample\_size}_i + \beta_3 \cdot \mathrm{recency}_i +  \beta_4 \cdot \mathrm{state}_i + \epsilon_i
$$

Where

-   $y_i$​ is the percentage of support for Kamala Harris in poll iii,

-   $α$ is the intercept,

-   $β_1$ captures the effect of the polling organization,

-   $β_2​$ captures the effect of the sample size,

-   $β_3$​ captures the effect of recency (how recent the poll is),

-   $β_4$ capture the effects of the different states

-   ​ $\epsilon_i$ represents the error term, assumed to follow a normal distribution with mean 0.

## Model Justification

We compared three models with different sets of predictors, gradually increasing the model complexity. Model 1 only included **pollster** and **sample size** as predictors, while Model 2 introduced **state** as an additional factor. Finally, Model 3 incorporated **recency** as another key predictor. Among these models, Model 3 demonstrated the highest $R^2$ value (0.731), indicating the best fit and ability to explain the variation in the data. Additionally, Model 3 had the lowest AIC (4786.815) and BIC (5203.787) values, suggesting that it provided the best balance between model complexity and predictive performance. Therefore, Model 3 was selected as the most appropriate model for smoothing out polling biases and differences, offering the most accurate prediction of support for Kamala Harris.

```{r}
#| warning: false
#| message: false
#| echo: false
# Load the models from RDS files
model_1 <- readRDS(here("models/model_1.rds"))
model_2 <- readRDS(here("models/model_2.rds"))
model_3 <- readRDS(here("models/model_3.rds"))

# Function to get model details
get_model_details <- function(model, model_name, include_recency = FALSE) {
  r2 <- summary(model)$r.squared
  adj_r2 <- summary(model)$adj.r.squared
  f_stat <- summary(model)$fstatistic[1]
  
  # Extract p-values for Sample Size and Recency
  sample_size_pvalue <- tidy(model) %>% filter(term == "sample_size") %>% pull(p.value)
  sample_size_significance <- ifelse(sample_size_pvalue < 0.05, paste0("Significant (p = ", round(sample_size_pvalue, 4), ")"), paste0("Not significant (p = ", round(sample_size_pvalue, 4), ")"))
  
  recency_significance <- "Not included"
  if (include_recency) {
    recency_pvalue <- tidy(model) %>% filter(term == "recency") %>% pull(p.value)
    recency_significance <- ifelse(recency_pvalue < 0.05, paste0("Highly significant (p < 2e-16)"), paste0("Not significant (p = ", round(recency_pvalue, 4), ")"))
  }
  
  return(data.frame(
    Model = model_name,
    Variables_Included = ifelse(include_recency, "Pollster + Sample Size + State + Recency", "Pollster + Sample Size + State"),
    R2 = round(r2, 3),
    Adjusted_R2 = round(adj_r2, 3),
    F_statistic = round(f_stat, 2),
    Sample_Size_Significance = sample_size_significance,
    Recency_Significance = recency_significance,
    States_Significant = ifelse(model_name == "Model 1", "Not included", ifelse(model_name == "Model 2", "Several states", "More states"))
  ))
}

# Generate the summaries for all models
model_1_summary <- get_model_details(model_1, "Model 1")
model_2_summary <- get_model_details(model_2, "Model 2")
model_3_summary <- get_model_details(model_3, "Model 3", include_recency = TRUE)

# Combine all summaries into one data frame
all_summaries <- bind_rows(model_1_summary, model_2_summary, model_3_summary)

# Print the table using kable for better formatting

kable(all_summaries, format = "markdown",row.names = FALSE, col.names = c("Model", "Variables Included", "R²", "Adjusted R²", "F-statistic", "Sample Size Significance", "Recency Significance", "States Significant"))


```

```{r}
#| warning: false
#| message: false
#| echo: false

# Create a data frame for AIC and BIC comparison
aic_bic_table <- data.frame(
  Model = c("Model 1", "Model 2", "Model 3"),
  AIC = c(AIC(model_1), AIC(model_2), AIC(model_3)),
  BIC = c(BIC(model_1), BIC(model_2), BIC(model_3))
)

# Print the table using kable for LaTeX format
kable(aic_bic_table, format = "markdown", col.names = c("Model", "AIC", "BIC"), booktabs = TRUE)

```

## Model Results

The table below shows the model output, where only a few states and pollsters are displayed to keep the output concise, as the full model includes many variables. We included "State California," "State Texas," "Pollster Redfield & Wilton Strategies," and "Pollster Echelon Insights" to provide examples of how different states and pollsters influence support for Kamala Harris.

The variable "Sample Size" has a positive coefficient, suggesting that polls with larger sample sizes are associated with slightly higher support percentages. "Recency" has a negative and highly significant coefficient, indicating that more recent polls tend to show lower support levels. Additionally, certain states like California and Texas show significant effects on the percentage of support, with California showing a notable increase. Similarly, different pollsters also display varying biases, as seen with the coefficients for "Pollster Redfield & Wilton Strategies" and "Pollster Echelon Insights." The model shows strong goodness-of-fit, with an $R^2$ of 0.731, indicating that 73.1% of the variability in support can be explained by the included variables.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false
# Load necessary package
library(stargazer)

# Use stargazer to display only the specified variables and custom labels
stargazer(model_3, 
          type = "text",  # Can be "text", "html", or "latex"
          covariate.labels = c("Sample Size", "Recency", "State California", "State Texas",
                               "Pollster Redfield & Wilton Strategies", "Pollster Echelon Insights"),  # Renaming the variables
          keep = c("sample_size", "recency", "stateCalifornia", "stateTexas", 
                   "pollsterRedfield & Wilton Strategies", "pollsterEchelon Insights"),  # Keep only these variables
          omit.stat = c("f", "ser"),  # Optionally omit F-statistics or other GOF stats
          star.cutoffs = c(0.05, 0.01, 0.001),  # Custom significance stars
          single.row = TRUE)  # Display coefficients on one row for readability

```

# Prediction

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false


# Create a new dataset called 'predict_data' with predicted values and weights
predict_data <- analysis_data %>%
  # Add the predicted values (fitted values from model_3)
  mutate(fitted_values = fitted(model_3),
         # Create weights based on numeric grade
         weight = numeric_grade / sum(numeric_grade))

# Calculate the weighted fitted values for Kamala Harris support
weighted_support <- sum(predict_data$fitted_values * predict_data$weight)



```

### Prediction

To predict Kamala Harris' overall support, we used a weighted average approach based on the quality of each poll. The weights are calculated using each poll's `numeric_grade`, which reflects the reliability and transparency of the polling methodology.

We define the weight for each pollster $w_i$ as follows:

$$
w_i = \frac{\mathrm{numeric\_grade}_i}{\sum_{i=1}^{n} \mathrm{numeric\_grade}_i}
$$

where:

-   $w_i$​ represents the weight assigned to poll i,

-   $numericgrade_i$ is the numeric grade of poll i, and

-   $n$ is the total number of polls used in the analysis.

Using these weights, the overall weighted prediction of Kamala Harris' support is calculated by summing the weighted predicted values from our regression model:

$$
\text{Overall Weighted Support} = \sum_{i=1}^{n} w_i \cdot \hat{y}_i
$$

-   $\hat{y}_i$​ is the predicted percentage of support for Kamala Harris from poll i.

Applying this approach, our model predicts Kamala Harris' overall support to be **47.78%**. This prediction indicates that, after adjusting for biases in the polls, Harris' support is slightly below the 50% threshold. The weighted approach accounts for the differences in poll reliability and thus provides a more accurate estimate than using raw averages of the polls.

This result suggests that Kamala Harris is facing some challenges in gaining majority support based on the current polling data. However, the model reflects the weighted average from multiple sources, smoothing out individual poll biases to offer a comprehensive view of her support landscape.

# Discussion

In this paper, we set out to predict Kamala Harris' support using a polls-of-polls approach. By aggregating multiple polls, we aimed to smooth out the inherent biases present in individual surveys and create a more accurate and reliable forecast. Our analysis was driven by a linear regression model that used key predictors, such as pollster, sample size, state, and recency of the polls. After obtaining predicted values from the model, we applied a weighting scheme based on the numeric grade of each pollster, reflecting the quality and reliability of the poll. By assigning higher weights to more reliable pollsters, we generated a final prediction for Kamala Harris' overall support, accounting for variations across different states and polling organizations.

One key insight from our analysis is the significance of poll recency. The negative and highly significant coefficient for recency indicates that **older** polls show lower support for Kamala Harris, while more recent polls tend to show higher support. This suggests that her popularity may be increasing over time, or that certain factors have shifted voter sentiment in her favor in more recent months. Inncorporating recency into the model also greatly improved its overall performance. Adding recency as a predictor significantly increased the model's explanatory power, raising the R² value and lowering both AIC and BIC scores compared to models that excluded it. This indicates that accounting for the timing of polls is essential to accurately predicting voter sentiment, as public opinion is highly dynamic and evolves rapidly in response to events. Including recency strengthens the model's ability to reflect these temporal shifts, leading to more reliable forecasts.

Our analysis also highlights the variability in support for Kamala Harris across different states and pollsters. For example, certain states like California and Texas exhibit distinct effects, with California significantly boosting Harris’ predicted support. Additionally, different pollsters show varying levels of bias, as seen with Redfield & Wilton Strategies and Echelon Insights, which have divergent effects on support levels. This underscores the geographic and methodological complexities in predicting election outcomes, demonstrating that voter preferences are not homogenous across regions or polling firms.

Despite the strengths of the polls-of-polls methodology, there are several limitations to our approach. First, our model assumes that the relationship between the predictors (pollster, sample size, state, recency) and support for Kamala Harris is linear, which may not always be the case. Non-linear trends or interactions between variables might exist that were not captured by the model. Additionally, while we weighted polls based on their numeric grade, this metric might not fully reflect the true accuracy or bias of each pollster. Moreover, we only included polls conducted after Harris' official candidacy announcement, potentially omitting relevant data from earlier in the election cycle that could provide a fuller picture.

Future research could delve into more sophisticated methods to capture non-linear relationships and interactions between variables. For instance, incorporating machine learning algorithms like random forests or neural networks could allow for more flexible modeling of polling data. Another area of improvement could be refining the pollster weighting mechanism by integrating other factors, such as the historical performance of the pollster or specific characteristics of the electorate surveyed. Additionally, conducting more in-depth analyses of key battleground states could reveal more granular insights into voter behavior, especially in regions where small shifts in support could have outsized impacts on election outcomes.

In conclusion, while our model provides valuable predictions of Kamala Harris' support, there is always room to enhance the predictive accuracy by refining methodologies and incorporating new data sources. The evolving nature of elections demands continuous adjustment of models to reflect changes in public opinion, candidate strategies, and electoral dynamics.

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Model details {#sec-model-details}

## Diagnostics

\newpage

# References
