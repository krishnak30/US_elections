---
title: Forcasting US Elections using Polls-of-Polls
subtitle: "My subtitle if needed"
author: 
  - Shamayla Durrin
  - Denise Chang 
  - Krishna Kumar
thanks: "Code and data are available at: [https://github.com/krishnak30/US_elections](https://github.com/krishnak30/US_elections)."

date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(here)
library(ggplot2)
analysis_data <- read_parquet(here("data/02-analysis_data/analysis_data.parquet"))
```

# Introduction

# Data

## Data Set Overview

add a summary statistics table here maybe?

```{r}
library(arrow)
analysis_data <- read_parquet(here::here("data/02-analysis_data/analysis_data.parquet"))

#head(analysis_data, 20)
colnames(analysis_data)
#print(nrow(analysis_data))
```

## Variables

The data set contains a wide range of variables that are essential for understanding polling outcomes. The following are key variables:

-   **`poll_id`**: Unique identifier for each poll conducted.

-   **`pollster`**: The polling organization that conducted the poll (e.g., YouGov, TIPP).

-   **`numeric_grade`**: A numeric rating indicating the pollster’s quality or reliability (e.g., 1.8).

-   **`pollscore`**: A score representing the reliability of the pollster in terms of error and bias.

-   **`state`**: The state where the poll was conducted (or "Not Applicable" for national polls).

-   **`start_date` / `end_date`**: Dates during which the poll was conducted.

-   **`national`**: A binary variable which determines where a poll was national (1) or state (0)

-   **`sample_size`**: The number of respondents in the poll.

-   **`population`**: The group surveyed (e.g., likely voters or registered voters).

-   **`candidate_name`**: The name of the candidate being polled (e.g., Kamala Harris, Donald Trump).

-   **`pct`**: The percentage of support for the candidate in the poll.

-   **`methodology`**: The method used to conduct the poll (e.g. online panel)

-   **`num_support`**: The number of people supporting a candidate in a poll

## High Level Data Cleaning

blah blah blah

## Summary Statistics

### 1) Average Pollster Quality by Poll Type (pollscore vs state/national): mean, median, mode, min, max

```{r}
#| warning: false
#| message: false
#| echo: false


library(dplyr)

# Generate summary statistics for national and state polls
summary_stats <- analysis_data %>%
  filter(!is.na(pollscore)) %>%  # Exclude rows with missing pollscore
  group_by(national) %>%         # Group by the binary 'national' column (1 for national, 0 for state)
  summarize(
    Count = n(),
    Min = round(min(pollscore), 2),
    `1st Qu.` = round(quantile(pollscore, 0.25), 2),
    Median = round(median(pollscore), 2),
    Mean = round(mean(pollscore), 2),
    `3rd Qu.` = round(quantile(pollscore, 0.75), 2),
    Max = round(max(pollscore), 2),
    .groups = 'drop'  # Drop grouping structure for cleaner output
  ) %>%
  # Rename 'national' to 'Poll Type' and replace values
  mutate(`Poll Type` = ifelse(national == 1, "National", "State")) %>%
  select(`Poll Type`, Count, Min, `1st Qu.`, Median, Mean, `3rd Qu.`, Max)  # Reorder columns

# Print the summary statistics
print(summary_stats)


```

As shown above, the summary statistics table **\[CROSS-REFERENCE HERE\]** offers an overview of poll scores for national and state polls, with 427 and 696 entries, respectively. Both categories feature a minimum poll score of -1.5, which a high level of validity and reliability. The first quartile shows national polls at -0.9, while state polls are slightly lower at -1.1. Median scores are -0.3 for national polls and -0.4 for state polls, suggesting that half of the polls in both categories yield moderately negative scores, reinforcing a trend of favorable sentiment. The mean score is -0.39 for national polls, indicating a less reliable perception compared to the -0.54 mean for state polls. The third quartile for national and state polls (-0.3) indicates that the upper 25% of scores are slightly above neutral. The maximum score of 1.7 for national polls reflects a notable negative perception for at least one candidate, while the maximum for state polls is 0.6. Overall, these statistics illustrate a strong reliability of pollsters considering their errors and bias.

### 2) Distribution of Sample Sizes

```{r}
#| warning: false
#| message: false
#| echo: false

library(ggplot2)
library(dplyr)
library(gridExtra) 

# Adjust the sample size range categories
analysis_data <- analysis_data %>%
  mutate(sample_size_range = case_when(
    sample_size <= 6000 ~ "0-6,000",
    sample_size > 6000 & sample_size <= 13000 ~ "6,001-13,000",
    TRUE ~ "Above 13,000"  # Catch any values above 13,000
  ))

# Plot for 0-6,000
p1 <- ggplot(analysis_data %>% filter(sample_size <= 6000), aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "lightgreen", color = NA, alpha = 0.7) +
  labs(title = "Sample Sizes (0-6,000)",
       x = "Sample Size",
       y = "Count") +
  scale_x_continuous(limits = c(0, 6000)) +  # Set x limits for the first plot
  scale_y_continuous(limits = c(0, 500)) +  # Adjust y limits based on data
  theme_minimal() +
  theme(plot.title = element_text(size = 10, hjust = 0.5),  # **Center the title**
        axis.title.y = element_text(margin = margin(r = 15)))  # **Increased gap for y-axis label**

# Plot for 6,001 - 13,000
p2 <- ggplot(analysis_data %>% filter(sample_size > 6000 & sample_size <= 13000), aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "indianred", color = NA, alpha = 0.7) +
  labs(title = "Sample Sizes (6,001-13,000)",
       x = "Sample Size",
       y = "Count") +
  scale_x_continuous(limits = c(6001, 13000)) +  # Set x limits for the second plot
  scale_y_continuous(limits = c(0, 20)) +  # Adjust y limits based on data
  theme_minimal() +
  theme(plot.title = element_text(size = 10, hjust = 0.5),  # **Center the title**
        axis.title.y = element_text(margin = margin(r = 10)))  # **Increased gap for y-axis label**

# Arrange the two plots side by side
grid.arrange(p1, p2, ncol = 2)  # ncol=2 arranges the plots in one row


```

The histograms **\[CROSS-REFERENCE HERE\]** reveal significant trends in sample sizes across the different ranges, indicating a general tendency towards smaller samples in the polling data. **In Plot 1**, most of the values are concentrated on the left side, with very few values extending beyond the 4,500 mark. This clustering suggests that many polls are conducted with limited respondent numbers, which may compromise the reliability and representativeness of the results.

In **Plot 2**, the values are somewhat evenly divided between 6,000 and the maximum sample size of 12,084. This distribution shows a broader representation of sample sizes, indicating that while there are still smaller samples, there are also a notable number of polls that feature larger sample sizes. This contrast with Plot 1 highlights a shift towards more robust polling efforts.

The findings from these histograms suggest potential limitations and advantages in the accuracy and validity of the insights generated from the polling data. Smaller sample sizes are typically less desirable in polling, as they can yield less reliable estimates and increase the margin of error. Consequently, the predominance of smaller sample sizes in Plot 1 raises concerns about the robustness of the polling findings. However, the more even distribution in Plot 2 suggests that some polls conducted with larger sample sizes could enhance the reliability of the results.

Ultimately, the analysis emphasizes the need to consider sample sizes critically when interpreting polling data, as larger sample sizes tend to provide a better representation of the population's views. Thus, while many polls may fall short of ideal sample sizes, the existence of larger samples in the dataset can offer valuable insights into public opinion. **However, we need to determine an ideal and acceptable sample size that would be enough for the polling results to be considered reliable, before we conclude smaller samples as being unreliable.**

### 3) Summary Statistics Table for Numeric Variables

```{r}
#| warning: false
#| message: false
#| echo: false


library(dplyr)
library(modelsummary)
library(tools)  # Make sure to load the tools package for toTitleCase()

# Set options to use kableExtra for table formatting
options(modelsummary_factory_default = 'kableExtra')

# Create the summary statistics table for the specified numerical variables
summary_table <- analysis_data %>%
  select(numeric_grade, pollscore, sample_size, num_support) %>%
  # Remove underscores and capitalize the first letter of each word
  rename_with(~ toTitleCase(gsub("_", " ", .))) %>%
  datasummary_skim(
    fun_numeric = list(
      Mean = mean,
      SD = sd,
      Min. = min,
      Median = median,
      Max. = max
    )
  )

# Display the summary statistics table
print(summary_table)

```

The summary statistics **\[CROSS REFERENCE HERE\]** for the numeric variables provide significant insights into the polling data. The **numeric grade** has a mean of 2.3 and a standard deviation of 0.6, suggesting that most polls are graded around a middle value, though there is moderate variability. The range, with a minimum of 1.1 and a maximum of 3.0, indicates the presence of both low-quality and higher-quality polls within the dataset.

The **pollscore** averages at −0.5, with a standard deviation of 0.6. Since lower poll scores indicate more favorable outcomes for certain candidates or parties, this negative mean suggests that the polls are, on average, skewing positively towards their targets. The minimum of −1.5 and maximum of 1.7 further illustrate the wide distribution of poll results, highlighting the variability in outcomes across different polls.

Looking at the **sample size**, the mean of 1962.5, accompanied by a substantial standard deviation of 2608.0, reflects significant variability in respondent numbers. With a minimum sample size of 382 and a maximum of 12,084, it is evident that while many polls are conducted with relatively small sample sizes, a few achieve much larger respondent pools. This variability can enhance the reliability of findings, particularly for those polls with larger sample sizes.

Finally, the **number of supporters** reported has a mean of 949.2 and a standard deviation of 1290.0, indicating that while the average number of supporters is relatively high, there is considerable variability. The minimum value of 172 and maximum of 6042 underscores the diverse contexts in which these polls were conducted.

Overall, these summary statistics emphasize the variability and range present in the polling data, which may impact the reliability and interpretation of the findings. The mixture of low and high values in numeric grade, pollscore, and sample size points to the importance of careful analysis when drawing conclusions from these polling efforts.

### 4) Top 10 Pollsters and their Average Pollscores and Numeric Grades

```{r}
library(dplyr)

# Get the top 10 pollsters by frequency along with their average pollscore and numeric grade
top_pollsters <- analysis_data %>%
  group_by(pollster) %>%           # Group by the 'pollster' variable
  summarize(
    Count = n(),                   # Count the number of occurrences
    Average_Pollscore = round(mean(pollscore, na.rm = TRUE), 2),  # Average pollscore
    Average_Numeric_Grade = round(mean(numeric_grade, na.rm = TRUE), 2),  # Average numeric grade
    .groups = 'drop'               # Drop grouping structure for cleaner output
  ) %>%
  arrange(desc(Count)) %>%         # Arrange in descending order of count
  slice_head(n = 10)                # Select the top 10 pollsters

# Rename the columns to remove underscores and capitalize 'Pollster'
colnames(top_pollsters) <- c("Pollster", "Count", "Average Pollscore", "Average Numeric Grade")

# Print the top pollsters table
print(top_pollsters)

```

The examination of the top 10 pollsters in **\[CROSS REFERENCE HERE\]** highlights their significance in electoral forecasting. **Morning Consult**, with the highest volume of polls (235), shows a slightly optimistic average pollscore of −0.3, indicating some reliability and validity. Conversely, **Siena/NYT** stands out with the lowest average pollscore of −1.5 and the highest numeric grade of 3.0, suggesting a rigorous methodology that enhances its credibility in predicting electoral outcomes.

In contrast, **Redfield & Wilton Strategies** displays an average pollscore of 0.4, which may create a misleadingly pessimistic view of voter sentiment. This underscores the necessity of considering multiple polling sources for a nuanced understanding of electoral dynamics.

The presence of several pollsters with negative scores indicates a general trend towards specific candidates, which is crucial for shaping campaign strategies. Overall, these findings reinforce the critical role of trustworthy polling in influencing public perception and political strategy during elections.

### 5) Histogram of End Dates

## Graphs for Imp. Variables (relationships)

## Measurement

Some paragraphs about how we go from a phenomena in the world to an entry in the dataset.

Public support for political candidates is an essential part of understanding electoral dynamics for a variety of reasons. Public polls provide insights into the preferences and beliefs of certain groups of people, usually voters in specific states or districts, and sometimes to also correlate preferences to demographic information (race, age, neighborhood, etc).

The results help inform policy decisions, help track the likely outcomes of elections, help government officials and candidates decide where to invest time, funds, and resources to get the desired results. Polling organizations aim to capture data by surveying a representative sample of voters, free from government intervention.

Polling organizations conduct these surveys through various methods. For our particular data set, the following methods have been used to gauge public sentiment:

-   App panel

-   Online panel

-   Email

-   Online ad

-   IVR (interactive voice response)

-   Live phone

-   Text

-   Text-to-web

-   Probability panel

-   Mail-to-phone

Each pollster chooses a method(s) based on what they find to be most convenient, efficient, and providing the best results. Moreover, there are some variables which affect the validity and reliability of the polling results, such as sample size, methodology, population type, etc. Larger samples generally lead to more reliable estimates of public opinion. Some methodologies might be considered more reliable due to certain factors. The pollsters also use different sampling methods (random, stratified, etc) which can affect the validity of the polling results as well.

After collecting the required data, the polling organizations transform these results into more easily comprehensible variables such as numeric grade, poll score, transparency score, and a percentage. These scores are calculated through an extensive process of calculating excess error and bias, adjusting for race difficulty, calculating predictive error and bias, calculating pollster transparency, and combining everything into one single ‘POLLSCORE’, while accounting for luck as well.

Variables such as pollster rating and transparency score help validate the reliability of the pollster and the polling results they publish.

Each entry in the data set represents a glimpse of public opinion at a specific time and location, capturing measured support percentages alongside contextual variables such as pollster, methodology, and sample size. This extensive approach allows for an enhanced understanding of public support dynamics, facilitating informed analyses of electoral outcomes.

# Polls of Polls to Forecast Winning Candidate

## Forecasting Approach

The polls of polls methodology is widely used in election prediction as it aggregates multiple polls to provide a more reliable estimate of voter support, rather than relying on any single poll. The goal is to reduce errors and biases present in individual polls by using a weighted average of many different polls. In our approach, we calculate the weights based on several important variables. Sample size is included because larger polls are generally more reliable and have smaller margins of error. Pollscore (or pollster rating) reflects the empirical quality of a pollster, balancing both accuracy and bias, allowing us to give more weight to those with a proven track record. We also account for the pollscope, differentiating between national and state-level polls, as state polls may offer more localized insights into voter sentiment. We also use recency as more recent polls give us a more accurate estimate of voter sentiment.

However, we acknowledge that even with these adjustments, there may still be underlying biases from individual pollsters toward the candidates. To account for this, we will select a baseline pollster with a strong rating, high transparency, and minimal historical error, and regress the support percentage (pct) on pollster, including the same variables as controls whcih was used in the weighting method, such as sample size, pollscore and pollscope.Additionally, we will control for the state of the poll, as support levels can vary regionally. Therefore each pollster's coefficient in the regression model will represent the bias of that pollster relative to the baseline pollster. Controlling for these factors is crucial because it ensures that the differences between pollsters are not driven by other factors, such as differences in sample size or the types of polls they conduct, but by their inherent bias in support estimates. Once we have calculated the weights, we will use them to generate a weighted average of support for Kamala Harris and Donald Trump, ultimately comparing their weighted support to forecast the likely winner of the election.

## **Exploration of Control Variables**

### Investigating the Effect of Pollscore on Candidate Support

```{r}

# Define pollscore ranges (you can adjust the binwidth if needed)
cleaned_data <- analysis_data %>%
  mutate(pollscore_range = cut(pollscore, breaks = seq(-1.5, 1, by = 0.3), include.lowest = TRUE))

# Calculate the average support for each candidate within each pollscore range
average_support <- cleaned_data %>%
  group_by(pollscore_range, candidate_name) %>%
  summarise(avg_support = mean(pct, na.rm = TRUE)) %>%
  ungroup()

# Create a bar plot
ggplot(average_support, aes(x = pollscore_range, y = avg_support, fill = candidate_name)) +
  geom_bar(stat = "identity", position = "dodge") +  # Dodge to separate bars for each candidate
  scale_fill_manual(values = c("Donald Trump" = "indianred", "Kamala Harris" = "steelblue")) +  # Custom colors
  labs(title = "Average Support for Candidates by Pollscore Range",
       x = "Pollscore Range",
       y = "Average Support (%)",
       fill = "Candidate") +
  theme_minimal() +  # Minimal theme
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5)
  )


```

### Investigating the Effect of Poll Recency on Candidate Support

```{r}
#| warning: false
#| message: false
#| echo: false
# Custom colors for candidates
trump_color <- "indianred"
harris_color <- "steelblue"

# Create the plot with the requested adjustments (recency vs support)
ggplot(analysis_data, aes(x = recency, y = pct, color = candidate_name)) +
  geom_point(size = 0.5, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) + # Smaller dots with slight transparency
  scale_color_manual(values = c("Donald Trump" = trump_color, "Kamala Harris" = harris_color)) +  # Custom colors
  theme_minimal() +  # Remove grey background
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.title = element_blank(),  # Remove "candidate_name" as title
    legend.position = "bottom",  # Move legend to the bottom
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine for Kamala Harris
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(title = "Recency vs Support for Kamala Harris and Donald Trump",
       x = "Poll Recency (days since poll end date)", 
       y = "Support (%)") +
  facet_wrap(~ candidate_name, scales = "free_y")  # Separate plots for each candidate

```

### Investigate the effect of sample size on support estimates

```{r}
#| warning: false
#| message: false
#| echo: false


# Custom colors for candidates
trump_color <- "indianred"
harris_color <- "steelblue"

# Create the plot with the requested adjustments
ggplot(analysis_data, aes(x = sample_size, y = pct, color = candidate_name)) +
  geom_point(size = 0.5, alpha = 0.9) + # Smaller dots with slight transparency
  geom_smooth(method = "lm", se = FALSE)+ 
 
  scale_color_manual(values = c("Donald Trump" = trump_color, "Kamala Harris" = harris_color)) +  # Custom colors
  theme_minimal() +  # Remove grey background
  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    legend.title = element_blank(),  # Remove "candidate_name" as title
    legend.position = "bottom",  # Move legend to the bottom
    plot.title = element_text(hjust = 0.5),  # Center the title
    axis.line = element_line(color = "black"),  # Add axis lines
    panel.border = element_blank(),  # Remove panel border
    axis.line.y.right = element_line(color = "black"),  # Add right spine for Kamala Harris
    axis.line.x.top = element_blank()  # Remove top spine
  ) +
  labs(title = "Sample Size vs Support for Kamala Harris and Donald Trump",
       x = "Sample Size", 
       y = "Support (%)") +
  facet_wrap(~ candidate_name, scales = "free_y")
```

### Distribution of support for Candidates by State

```{r}
# Calculate mean support for each candidate in each state
state_support <- analysis_data %>%
  group_by(state, candidate_name) %>%
  summarise(mean_support = mean(pct)) %>%
  ungroup()

# Pivot the data to have one row per state with separate columns for Harris and Trump
state_support_wide <- state_support %>%
  pivot_wider(names_from = candidate_name, values_from = mean_support)
```

```{r}
# Calculate mean support for each candidate in each state
state_support <- analysis_data %>%
  group_by(state, candidate_name) %>%
  summarise(mean_support = mean(pct))

# Calculate the proportion of support for Kamala Harris
state_support <- state_support %>%
  mutate(harris_proportion = Kamala_Harris / (Kamala_Harris + Donald_Trump))

```

# Model

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.

### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.

# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <-
  readRDS(file = here::here("models/first_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```

# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this.

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows...

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```

\newpage

# References
